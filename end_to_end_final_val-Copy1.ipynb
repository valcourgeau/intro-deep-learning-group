{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization, ConvLSTM2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wavelet functions\n",
    "# Definining variables for WT\n",
    "# this is the wavelet creation call part...\n",
    "current_level = 2\n",
    "current_mode = 'constant'\n",
    "current_wavelet = pywt.Wavelet('haar')\n",
    "\n",
    "def save_as_csv(data_array, file_name=\"data\"):\n",
    "    output_name = file_name + \".csv\"\n",
    "    np.savetxt(output_name, data_array, delimiter=\",\", fmt='%-7.4f', \n",
    "               header=str(data_array.dtype.names).replace('(',\"\").replace(')',\"\").replace(\"#\", \"\") + \" \",\n",
    "              comments='')\n",
    "    \n",
    "def apply_wt_once(data, wavelet='haar', level=2, mode='constant'):\n",
    "    haar_dwt = pywt.wavedec(data, wavelet=wavelet, level=level, mode=mode)\n",
    "    \n",
    "    # delete the high frequencies from the decomposition\n",
    "    #haar_dwt[1] = np.zeros_like(haar_dwt[1])\n",
    "    haar_dwt[2] = np.zeros_like(haar_dwt[2])\n",
    "  \n",
    "    return pywt.waverec(haar_dwt, mode=mode, wavelet=wavelet)\n",
    "\n",
    "def apply_wt_twice(data, wavelet='haar', level=2, mode='constant'):\n",
    "    wt_results = apply_wt_once(data=data,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "    \n",
    "    return apply_wt_once(data=wt_results,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "\n",
    "def get_accuracy(data1, data2):\n",
    "    return np.std(data1-data2)\n",
    "\n",
    "def apply_wt_twice_on_dataset(dataset, wavelet='haar', level=2, mode='constant'):\n",
    "    #ignored_col_names = ('Ntime', 'time', 'Time') # from the SP500 dataset, might need some tweaking\n",
    "    col_names = dataset.dtype.names\n",
    "    results = np.zeros_like(dataset)\n",
    "\n",
    "    #results[ignored_col_names[0]] = dataset[ignored_col_names[0]]\n",
    "    #if ignored_col_names[1] in dataset.dtype.names:\n",
    "    #    results[ignored_col_names[1]] = dataset[ignored_col_names[1]]\n",
    "    #else:\n",
    "    #    results[ignored_col_names[2]] = dataset[col_names[1]]\n",
    "    \n",
    "    for name in col_names:\n",
    "        #if name not in ignored_col_names:\n",
    "        temp = apply_wt_twice(\n",
    "                            dataset[name],\n",
    "                            wavelet=wavelet,\n",
    "                            level=level,\n",
    "                            mode=mode\n",
    "                             )\n",
    "        results[name] = temp[:len(dataset[name])]\n",
    "    return results\n",
    " \n",
    "def end_to_end_twice_wt_with_csv(input_file_name, output_file_name, wavelet='haar', level=2, mode='constant'):\n",
    "    data_array = np.genfromtxt('data/' + input_file_name, delimiter=',', dtype=float, names=True)\n",
    "    res = apply_wt_twice_on_dataset(data_array,\n",
    "                                wavelet=wavelet,\n",
    "                                level=level,\n",
    "                                mode=mode)\n",
    "    save_as_csv(res, file_name=\"data/data_wt/\" + output_file_name)\n",
    "    res = [list(x) for x in res]\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "#Autoencoder functions\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                       activation, learning_rate = 0.005, l2_reg = 0.0005,\n",
    "                      seed=42):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            activation=activation,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=None, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                #print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]\n",
    "\n",
    "\n",
    "#normalise dataset pre and post autoencoder input\n",
    "def normalise_dataset(dataset):\n",
    "    n_cols = dataset.shape[1]\n",
    "    norm_dataset = np.zeros_like(dataset)\n",
    "    for i in range(n_cols): \n",
    "        mean = np.sum(dataset[:,i])/len(dataset[:,i])\n",
    "        std_dev = np.dot(dataset[:,i]-mean, dataset[:,i]-mean)/(len(dataset[:,i]) - 1)\n",
    "        std_dev = np.sqrt(std_dev)\n",
    "        norm_dataset[:,i] = (dataset[:,i] - mean) / std_dev\n",
    "\n",
    "    return norm_dataset\n",
    "\n",
    "# conversion code to create lookbacks for lstms\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(trainX, trainY, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(trainX)-look_back-1):\n",
    "        a = trainX[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(trainY[i + look_back-1])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#Train the stacked autoencoder\n",
    "#then take the latent layer\n",
    "\n",
    "def sae(x_train, N_EPOCHS, BATCH_SIZE,LEARNING_RATE, neurons):\n",
    "    reset_graph()\n",
    "    ACTIVATION = tf.nn.tanh\n",
    "\n",
    "    hidden_output1, W1, b1, W8, b8 = train_autoencoder(x_train, n_neurons=neurons[0], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output2, W2, b2, W7, b7 = train_autoencoder(hidden_output1, n_neurons=neurons[1], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output3, W3, b3, W6, b6 = train_autoencoder(hidden_output2, n_neurons=neurons[2], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    o4, W4, b4, W5, b5 = train_autoencoder(hidden_output3,n_neurons=neurons[3], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "\n",
    "\n",
    "    #Extract the latent outputs from the stacked ae\n",
    "    reset_graph()\n",
    "    activation = ACTIVATION\n",
    "\n",
    "    n_inputs = 1*(x_train.shape[1])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    \n",
    "    hidden1 = activation(tf.matmul(X, W1) + b1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, W2) + b2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, W3) + b3)\n",
    "    hidden4 = activation(tf.matmul(hidden3, W4) + b4)\n",
    "    hidden5 = activation(tf.matmul(hidden4, W5) + b5)\n",
    "    hidden6 = activation(tf.matmul(hidden5, W6) + b6)\n",
    "    hidden7 = activation(tf.matmul(hidden6, W7) + b7)\n",
    "    \n",
    "    \n",
    "    #hidden1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    #hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2)\n",
    "    #hidden3 = tf.nn.relu(tf.matmul(hidden2, W3) + b3)\n",
    "    #hidden4 = tf.nn.relu(tf.matmul(hidden3, W4) + b4)\n",
    "    #hidden5 = tf.nn.relu(tf.matmul(hidden4, W5) + b5)\n",
    "    #hidden6 = tf.nn.relu(tf.matmul(hidden5, W6) + b6)\n",
    "    #hidden7 = tf.nn.relu(tf.matmul(hidden6, W7) + b7)\n",
    "    outputs = tf.matmul(hidden7, W8) + b8\n",
    "\n",
    "    #unnecessary - should be the same as o4 but just to check\n",
    "    with tf.Session() as sess:\n",
    "            latent_val = hidden4.eval(feed_dict={X: x_train})\n",
    "\n",
    "    # Normalise output of SAEs\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "    # Save output in .csv file\n",
    "    latent_out = pd.DataFrame(latent_val) #latent_val\n",
    "    latent_out.to_csv('data/auto_out.csv', index=False)\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    return latent_val  \n",
    "\n",
    "#for the moment just a holder function for running the lstm or mlp\n",
    "#can tweak parameters depending on tests\n",
    "\n",
    "def run_regression(trainX, trainY, testX, testY, look_back, LEARNING_RATE, BATCH_SIZE, EPOCHS, neurons, dropout, decay=0.0):\n",
    "    #reset_graph()\n",
    "    K.clear_session()\n",
    "    # create and fit the LSTM network\n",
    "    \n",
    "    opti_adam = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    features = trainX.shape[1]\n",
    "    lstm_features = trainX.shape[-1]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #lstm test\n",
    "    if look_back != 0:\n",
    "        for i in enumerate(neurons):\n",
    "            if i[0] != len(neurons)-1:\n",
    "                model.add(LSTM(i[1], input_shape=(look_back, lstm_features),  return_sequences=True, unroll=True))\n",
    "                model.add((Dropout(dropout)))\n",
    "                model.add(BatchNormalization())\n",
    "            else:\n",
    "                model.add(LSTM(i[1]))\n",
    "                model.add((Dropout(dropout)))\n",
    "                model.add(BatchNormalization())\n",
    "    #MLP Test\n",
    "    else:\n",
    "        for i in enumerate(neurons): \n",
    "            model.add(Dense(i[1], input_dim=features, activation= 'relu'))\n",
    "            model.add((Dropout(dropout)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=opti_adam)\n",
    "    #history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "    #                    verbose=1, callbacks=[plot_losses, EarlyStopping(monitor='val_loss', patience=5)], \n",
    "    #                    validation_data=(testX, testY)) #validation_split = 0.1)\n",
    "    \n",
    "    history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "              verbose=1, callbacks=[plot_losses], validation_data=(testX, testY))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "#this is to create data set inputs to the mlp or lstm\n",
    "def make_reg_data(latent_val, y_train, look_back):\n",
    "\n",
    "    features = latent_val.shape[1]\n",
    "    all_trainX = latent_val \n",
    "\n",
    "    trainX = all_trainX[:round(0.9*len(all_trainX))]\n",
    "    testX = all_trainX[round(0.9*len(all_trainX)):]\n",
    "\n",
    "    all_trainY = np.array(y_train[:-1])\n",
    "    trainY = all_trainY[:round(0.9*len(all_trainY))]\n",
    "    testY = all_trainY[round(0.9*len(all_trainY)):]\n",
    "    \n",
    "    if look_back != 0:\n",
    "        ##### for an mlp dont do the lookback stuff ######\n",
    "        trainX, trainY = create_dataset(trainX, trainY, look_back)\n",
    "        testX, testY = create_dataset(testX, testY, look_back) \n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "#Function to enable live graph of losses as training progresses \n",
    "#note it is called in this cell also and used later\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Live plot for loss during training\n",
    "# Taken from: https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Reset the plot to only have the last 100 epochs\n",
    "        if len(self.x) % 100 == 0:\n",
    "            self.on_train_begin()\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "#examine initial features\n",
    "def make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac):\n",
    "    #including the creation of the wavelet stuff\n",
    "    #returns normalised X, and target y, currently hard coded files\n",
    "    #for targets it assumes a close price for the input file\n",
    "    #also for now hard coded to data/data_wt\n",
    "    \n",
    "    raw_data = pd.read_csv(\"data/\" + input_file)\n",
    "\n",
    "    #set up with complete cheat features to check it can learn - an MLP does what about lSTM\n",
    "    cheat_data = raw_data[features].copy() # raw_data[['Ntime', 'time', 'Close Price','Open Price']].copy()\n",
    "    if cheat:\n",
    "        cheat_data[\"Cheat\"] = raw_data['Close Price'].shift(-1)\n",
    "    cheat_data = cheat_data[:-1] #to keep it clean will take the last off whether cheat or not - because of y's\n",
    "    #print(cheat_data.head())\n",
    "    cheat_data.to_csv('data/cheat.csv', index=False)\n",
    "    \n",
    "    #run Valentin's wavelet stuff\n",
    "    ok = end_to_end_twice_wt_with_csv(\"cheat.csv\", \"cheat500_wt\")\n",
    "\n",
    "    if wavelet:\n",
    "        x_train = np.genfromtxt('data/data_wt/cheat500_wt.csv', delimiter=',', dtype=None, names=True)\n",
    "    else:\n",
    "        x_train = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "    x_train = [list(item) for item in x_train]\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    #set up target and test y's hard coded target price\n",
    "    y_lazy = pd.read_csv(\"data/\"+input_file)['Close Price'] \n",
    "    y_train = y_lazy.shift(-1)\n",
    "    \n",
    "    #because of the shift the last value will be n/a thus...\n",
    "    x_train = x_train[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "\n",
    "    # Normalised dataset\n",
    "    if normalise:\n",
    "        x_train = normalise_dataset(x_train)\n",
    "        \n",
    "    if cheat:\n",
    "        x_train[:,-1] = x_train[:,-1] +np.random.randn(x_train.shape[0])*cheat_fac\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "### Adding goodness of fit measures\n",
    "# MAPE\n",
    "def compute_mape(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    return np.sum(np.abs((y-y_pred) / y)) / len(y)\n",
    "\n",
    "def compute_r(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    return  np.dot(y - mean_pred, y_pred - mean_pred) / np.sqrt(np.sum(np.square(y - mean_pred)) * np.sum(np.square(y_pred - mean_pred)))\n",
    "\n",
    "def compute_theil_u(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    \n",
    "    n = len(y)\n",
    "    temp = np.sqrt(np.sum(np.square(y)) / n)\n",
    "    temp += np.sqrt(np.sum(np.square(y_pred)) / n)\n",
    "    temp = np.sqrt(np.sum(np.square(y - y_pred)) / n) / temp \n",
    "    return temp\n",
    "\n",
    "# Profitability\n",
    "\n",
    "BUY_COST = 0.05/100\n",
    "SELL_COST = 0.05/100\n",
    "\n",
    "def daily_pnl(y_tomorrow, y_today, buy=True, buy_cost=BUY_COST, sell_cost=SELL_COST):\n",
    "    if(buy):\n",
    "        return (y_tomorrow - y_today - (sell_cost*y_tomorrow + buy_cost*y_today)) / y_today\n",
    "    else:\n",
    "        return (y_today - y_tomorrow - (buy_cost*y_today + sell_cost*y_tomorrow)) / y_today\n",
    "    \n",
    "\n",
    "def buy_and_sell(y_truth, y_pred):\n",
    "    assert len(y_pred) == len(y_truth)\n",
    "    n = len(y_pred)\n",
    "    # Vector of 1s for buy, 0s for sell. Approx: no exact predictions...\n",
    "    buy_sell = [1 if y_pred[index+1] > y_truth[index] else -1 if y_pred[index+1] < y_truth[index] else 0 for index in range(n-1)]\n",
    "    ret = [daily_pnl(y_truth[index+1], y_truth[index]) if buy_sell[index] == 1 else\n",
    "           daily_pnl(y_truth[index+1], y_truth[index], buy=False) for index in range(n-1)]\n",
    "    return ret # np.sum(ret)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_set_regressions(params_list, folder_path):\n",
    "    output_list = []   \n",
    "    for param in params_list:\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "        if param['lookback'] != 0:\n",
    "            layers_to_neurons =  param['LSTM_neurons']\n",
    "        else:\n",
    "            layers_to_neurons =  param['MLP_neurons']\n",
    "\n",
    "        #get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "        x_train, y_train = make_raw_data(param['input_file'], param['features'], param['wavelet'], \n",
    "                                         param['normalise'], param['cheat'], param['cheat_fac'])\n",
    "        #then get the output from the autoencoder\n",
    "        if param['auto']:\n",
    "            latent_val = sae(x_train, param['N_EPOCHS'], param['BATCH_SIZE'], \n",
    "                             param['LEARNING_RATE'], param['neurons']) #the output is normalised\n",
    "        else:\n",
    "            latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "            latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "        trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, param['lookback'])   \n",
    "        \n",
    "        model, history = run_regression(trainX, trainY, testX, testY, \n",
    "                                        param['lookback'], param['LEARNING_RATE'], param['BATCH_SIZE_LSTM'], \n",
    "                                        param['EPOCHS'], layers_to_neurons, param['dropout'])\n",
    "        end_time = start_time - time.time()\n",
    "        predY = model.predict(testX)[:,0]\n",
    "        \n",
    "        model.summary()\n",
    "        performance_metrics = {'MAPE': compute_mape(testY, predY),\n",
    "                              'R': compute_r(testY, predY),\n",
    "                              'theilU': compute_theil_u(testY, predY),\n",
    "                              'buy_and_sell': buy_and_sell(testY, predY),\n",
    "                               'preparation_and_training_time':end_time,\n",
    "                              'testY': np.array(testY),\n",
    "                              'predY': np.array(predY)}\n",
    "        \n",
    "        # Saving everything\n",
    "        model.save(folder_save + param['model_name'] + \".h5\")\n",
    "        np.save(folder_save + param['model_name'] + \".npy\", performance_metrics)\n",
    "        \n",
    "        with open(folder_save + param['model_name'] + \".yml\", 'w') as outfile:\n",
    "            yaml.dump(performance_metrics, outfile, default_flow_style=False)\n",
    "        \n",
    "        output_list.append((param, model, history, performance_metrics))\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhgTEBYoxhhjAuL/AxS0wNThSiATAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14b71b588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1864/1864 [==============================] - 1s - loss: 294265.7399 - val_loss: 245033.5011"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "reg_list = \"reg_list.yml\"\n",
    "folder_save = \"prediction_results/\"\n",
    "\n",
    "with open(\"data/\" + reg_list, 'r') as stream:\n",
    "    try:\n",
    "        regression_dict = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "reg_list = []\n",
    "for reg in regression_dict:\n",
    "    temp_reg = {}\n",
    "    for x in regression_dict[reg]:\n",
    "        temp_reg['model_name'] = reg\n",
    "        for key, value in x.items():\n",
    "            temp_reg[key] = value\n",
    "    reg_list.append(temp_reg)\n",
    "    \n",
    "run_set_regressions(reg_list, folder_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res1 = yaml.load(\"/Users/johngoodacre/intro-deep-learning-group/prediction_results/sp500_wt_sae_all_features.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/johngoodacre/intro-deep-learning-group/prediction_results/sp500_wt_sae_all_features.yml\", 'r') as stream:\n",
    "    try:\n",
    "        test_dict = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abs(test_dict['predY']-test_dict['testY'])/test_dict['predY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_dict['predY'])\n",
    "plt.plot(test_dict['testY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumprod(1+np.array(test_dict['buy_and_sell']))*100)\n",
    "plt.plot(test_dict['testY']/test_dict['testY'][0]*100)\n",
    "plt.title('Cumulative return v long only')\n",
    "plt.xlabel('Days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
