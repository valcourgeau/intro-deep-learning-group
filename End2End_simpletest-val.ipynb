{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization, ConvLSTM2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Wavelet functions\n",
    "# Definining variables for WT\n",
    "# this is the wavelet creation call part...\n",
    "current_level = 2\n",
    "current_mode = 'constant'\n",
    "current_wavelet = pywt.Wavelet('haar')\n",
    "\n",
    "def save_as_csv(data_array, file_name=\"data\"):\n",
    "    output_name = file_name + \".csv\"\n",
    "    np.savetxt(output_name, data_array, delimiter=\",\", fmt='%-7.4f', \n",
    "               header=str(data_array.dtype.names).replace('(',\"\").replace(')',\"\").replace(\"#\", \"\") + \" \",\n",
    "              comments='')\n",
    "    \n",
    "def apply_wt_once(data, wavelet='haar', level=2, mode='constant'):\n",
    "    haar_dwt = pywt.wavedec(data, wavelet=wavelet, level=level, mode=mode)\n",
    "    \n",
    "    # delete the high frequencies from the decomposition\n",
    "    #haar_dwt[1] = np.zeros_like(haar_dwt[1])\n",
    "    haar_dwt[2] = np.zeros_like(haar_dwt[2])\n",
    "  \n",
    "    return pywt.waverec(haar_dwt, mode=mode, wavelet=wavelet)\n",
    "\n",
    "def apply_wt_twice(data, wavelet='haar', level=2, mode='constant'):\n",
    "    wt_results = apply_wt_once(data=data,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "    \n",
    "    return apply_wt_once(data=wt_results,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "\n",
    "def get_accuracy(data1, data2):\n",
    "    return np.std(data1-data2)\n",
    "\n",
    "def apply_wt_twice_on_dataset(dataset, wavelet='haar', level=2, mode='constant'):\n",
    "    #ignored_col_names = ('Ntime', 'time', 'Time') # from the SP500 dataset, might need some tweaking\n",
    "    col_names = dataset.dtype.names\n",
    "    results = np.zeros_like(dataset)\n",
    "\n",
    "    #results[ignored_col_names[0]] = dataset[ignored_col_names[0]]\n",
    "    #if ignored_col_names[1] in dataset.dtype.names:\n",
    "    #    results[ignored_col_names[1]] = dataset[ignored_col_names[1]]\n",
    "    #else:\n",
    "    #    results[ignored_col_names[2]] = dataset[col_names[1]]\n",
    "    \n",
    "    for name in col_names:\n",
    "        #if name not in ignored_col_names:\n",
    "        temp = apply_wt_twice(\n",
    "                            dataset[name],\n",
    "                            wavelet=wavelet,\n",
    "                            level=level,\n",
    "                            mode=mode\n",
    "                             )\n",
    "        results[name] = temp[:len(dataset[name])]\n",
    "    return results\n",
    " \n",
    "def end_to_end_twice_wt_with_csv(input_file_name, output_file_name, wavelet='haar', level=2, mode='constant'):\n",
    "    data_array = np.genfromtxt('data/' + input_file_name, delimiter=',', dtype=float, names=True)\n",
    "    res = apply_wt_twice_on_dataset(data_array,\n",
    "                                wavelet=wavelet,\n",
    "                                level=level,\n",
    "                                mode=mode)\n",
    "    save_as_csv(res, file_name=\"data/data_wt/\" + output_file_name)\n",
    "    res = [list(x) for x in res]\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Autoencoder functions\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                       activation, learning_rate = 0.005, l2_reg = 0.0005,\n",
    "                      seed=42):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            activation=activation,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=None, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                #print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalise dataset pre and post autoencoder input\n",
    "def normalise_dataset(dataset):\n",
    "    n_cols = dataset.shape[1]\n",
    "    norm_dataset = np.zeros_like(dataset)\n",
    "    for i in range(n_cols): \n",
    "        mean = np.sum(dataset[:,i])/len(dataset[:,i])\n",
    "        std_dev = np.dot(dataset[:,i]-mean, dataset[:,i]-mean)/(len(dataset[:,i]) - 1)\n",
    "        std_dev = np.sqrt(std_dev)\n",
    "        norm_dataset[:,i] = (dataset[:,i] - mean) / std_dev\n",
    "\n",
    "    return norm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conversion code to create lookbacks for lstms\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(trainX, trainY, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(trainX)-look_back-1):\n",
    "        a = trainX[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(trainY[i + look_back-1])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train the stacked autoencoder\n",
    "#then take the latent layer\n",
    "\n",
    "def sae(x_train, N_EPOCHS, BATCH_SIZE,LEARNING_RATE, neurons):\n",
    "    reset_graph()\n",
    "    ACTIVATION = tf.nn.tanh\n",
    "\n",
    "    hidden_output1, W1, b1, W8, b8 = train_autoencoder(x_train, n_neurons=neurons[0], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output2, W2, b2, W7, b7 = train_autoencoder(hidden_output1, n_neurons=neurons[1], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output3, W3, b3, W6, b6 = train_autoencoder(hidden_output2, n_neurons=neurons[2], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    o4, W4, b4, W5, b5 = train_autoencoder(hidden_output3,n_neurons=neurons[3], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "\n",
    "\n",
    "    #Extract the latent outputs from the stacked ae\n",
    "    reset_graph()\n",
    "    activation = ACTIVATION\n",
    "\n",
    "    n_inputs = 1*(x_train.shape[1])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    \n",
    "    hidden1 = activation(tf.matmul(X, W1) + b1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, W2) + b2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, W3) + b3)\n",
    "    hidden4 = activation(tf.matmul(hidden3, W4) + b4)\n",
    "    hidden5 = activation(tf.matmul(hidden4, W5) + b5)\n",
    "    hidden6 = activation(tf.matmul(hidden5, W6) + b6)\n",
    "    hidden7 = activation(tf.matmul(hidden6, W7) + b7)\n",
    "    \n",
    "    \n",
    "    #hidden1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    #hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2)\n",
    "    #hidden3 = tf.nn.relu(tf.matmul(hidden2, W3) + b3)\n",
    "    #hidden4 = tf.nn.relu(tf.matmul(hidden3, W4) + b4)\n",
    "    #hidden5 = tf.nn.relu(tf.matmul(hidden4, W5) + b5)\n",
    "    #hidden6 = tf.nn.relu(tf.matmul(hidden5, W6) + b6)\n",
    "    #hidden7 = tf.nn.relu(tf.matmul(hidden6, W7) + b7)\n",
    "    outputs = tf.matmul(hidden7, W8) + b8\n",
    "\n",
    "    #unnecessary - should be the same as o4 but just to check\n",
    "    with tf.Session() as sess:\n",
    "            latent_val = hidden4.eval(feed_dict={X: x_train})\n",
    "\n",
    "    # Normalise output of SAEs\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "    # Save output in .csv file\n",
    "    latent_out = pd.DataFrame(latent_val) #latent_val\n",
    "    latent_out.to_csv('data/auto_out.csv', index=False)\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    return latent_val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for the moment just a holder function for running the lstm or mlp\n",
    "#can tweak parameters depending on tests\n",
    "\n",
    "def run_regression(trainX, trainY, testX, testY, look_back, LEARNING_RATE, BATCH_SIZE, EPOCHS, neurons, dropout, decay=0.0):\n",
    "    #reset_graph()\n",
    "    K.clear_session()\n",
    "    # create and fit the LSTM network\n",
    "    \n",
    "    opti_adam = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    features = trainX.shape[1]\n",
    "    lstm_features = trainX.shape[-1]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #lstm test\n",
    "    if look_back != 0:\n",
    "        for i in enumerate(neurons):\n",
    "            if i[0] != len(neurons)-1:\n",
    "                model.add(LSTM(i[1], input_shape=(look_back, lstm_features),  return_sequences=True, unroll=True))\n",
    "                model.add((Dropout(dropout)))\n",
    "                model.add(BatchNormalization())\n",
    "            else:\n",
    "                model.add(LSTM(i[1]))\n",
    "                model.add((Dropout(dropout)))\n",
    "                model.add(BatchNormalization())\n",
    "    #MLP Test\n",
    "    else:\n",
    "        for i in enumerate(neurons): \n",
    "            model.add(Dense(i[1], input_dim=features, activation= 'relu'))\n",
    "            model.add((Dropout(dropout)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=opti_adam)\n",
    "    history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                        verbose=1, callbacks=[plot_losses, EarlyStopping(monitor='val_loss', patience=5)], \n",
    "                        validation_data=(testX, testY)) #validation_split = 0.1)\n",
    "    \n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this is to create data set inputs to the mlp or lstm\n",
    "def make_reg_data(latent_val, y_train, look_back):\n",
    "\n",
    "    features = latent_val.shape[1]\n",
    "    all_trainX = latent_val \n",
    "\n",
    "    trainX = all_trainX[:round(0.9*len(all_trainX))]\n",
    "    testX = all_trainX[round(0.9*len(all_trainX)):]\n",
    "\n",
    "    all_trainY = np.array(y_train[:-1])\n",
    "    trainY = all_trainY[:round(0.9*len(all_trainY))]\n",
    "    testY = all_trainY[round(0.9*len(all_trainY)):]\n",
    "    \n",
    "    if look_back != 0:\n",
    "        ##### for an mlp dont do the lookback stuff ######\n",
    "        trainX, trainY = create_dataset(trainX, trainY, look_back)\n",
    "        testX, testY = create_dataset(testX, testY, look_back) \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to enable live graph of losses as training progresses \n",
    "#note it is called in this cell also and used later\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Live plot for loss during training\n",
    "# Taken from: https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Reset the plot to only have the last 100 epochs\n",
    "        if len(self.x) % 100 == 0:\n",
    "            self.on_train_begin()\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#examine initial features\n",
    "def make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac):\n",
    "    #including the creation of the wavelet stuff\n",
    "    #returns normalised X, and target y, currently hard coded files\n",
    "    #for targets it assumes a close price for the input file\n",
    "    #also for now hard coded to data/data_wt\n",
    "    \n",
    "    raw_data = pd.read_csv(\"data/\" + input_file)\n",
    "\n",
    "    #set up with complete cheat features to check it can learn - an MLP does what about lSTM\n",
    "    cheat_data = raw_data[features].copy() # raw_data[['Ntime', 'time', 'Close Price','Open Price']].copy()\n",
    "    if cheat:\n",
    "        cheat_data[\"Cheat\"] = raw_data['Close Price'].shift(-1)\n",
    "    cheat_data = cheat_data[:-1] #to keep it clean will take the last off whether cheat or not - because of y's\n",
    "    #print(cheat_data.head())\n",
    "    cheat_data.to_csv('data/cheat.csv', index=False)\n",
    "    \n",
    "    #run Valentin's wavelet stuff\n",
    "    ok = end_to_end_twice_wt_with_csv(\"cheat.csv\", \"cheat500_wt\")\n",
    "\n",
    "    if wavelet:\n",
    "        x_train = np.genfromtxt('data/data_wt/cheat500_wt.csv', delimiter=',', dtype=None, names=True)\n",
    "    else:\n",
    "        x_train = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "    x_train = [list(item) for item in x_train]\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    #set up target and test y's hard coded target price\n",
    "    y_lazy = pd.read_csv(\"data/\"+input_file)['Close Price'] \n",
    "    y_train = y_lazy.shift(-1)\n",
    "    \n",
    "    #because of the shift the last value will be n/a thus...\n",
    "    x_train = x_train[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "\n",
    "    # Normalised dataset\n",
    "    if normalise:\n",
    "        x_train = normalise_dataset(x_train)\n",
    "        \n",
    "    if cheat:\n",
    "        x_train[:,-1] = x_train[:,-1] +np.random.randn(x_train.shape[0])*cheat_fac\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####start here - set up our various parameter and design choices ###\n",
    "\n",
    "\n",
    "#the column list available here is ...\n",
    "\"\"\"['Ntime', 'time', 'Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "       'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "       'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\"\"\"\n",
    "\n",
    "input_file = \"sp500_index_data.csv\"\n",
    "\n",
    "#select which features we are going to examine\n",
    "features = ['Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "           'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "           'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\n",
    "cheat = False #do we include future information as an extra feature to prove and tune these things learning\n",
    "#if we do then we can noise it up to make it less of an oracle - this is a multiplier of gaussian noise added to the\n",
    "#last column of training data (where we put our cheat in) - note the data is normalised by this point\n",
    "#0.0 is perfect foresight, as the fac goes up, so does the noise until its a useless feature\n",
    "cheat_fac = 0.0 \n",
    "normalise = True #normalise data\n",
    "\n",
    "wavelet = True #use wavelet transform\n",
    "auto = False #use autoenconder - if no to both the raw normalised data goes into the lstm or mlp\n",
    "lookback = 4 #if 0 an MLP else the lookback period for an lstm\n",
    "dropout=0.2 #dropout parameter for generalisation tuning\n",
    "\n",
    "#autoencoder settings using relu activation\n",
    "N_EPOCHS=1000\n",
    "neurons = [20,15,15,10] #number of neurons for each layer of the stacked autoencoder\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.005\n",
    "decay = 1e-5\n",
    "\n",
    "#LSTM or MLP Settings\n",
    "BATCH_SIZE_LSTM = 64\n",
    "EPOCHS = 500\n",
    "decay = 1e-4\n",
    "LSTM_neurons = [len(features), len(features), len(features), len(features)]\n",
    "MLP_neurons = [20,10]\n",
    "\n",
    "if lookback!=0:\n",
    "    layer1_2neurons =  LSTM_neurons\n",
    "else:\n",
    "    layer1_2neurons =  MLP_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "x_train, y_train = make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac)\n",
    "\n",
    "#then get the output from the autoencoder\n",
    "if auto:\n",
    "    latent_val = sae(x_train, N_EPOCHS, BATCH_SIZE, LEARNING_RATE, neurons) #the output is normalised\n",
    "else:\n",
    "    latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "    \n",
    "    \n",
    "#here we create our final data for input to the lstm or mlp\n",
    "#clearly structure different for both\n",
    "#we pass all latent values it will chop into train/ test\n",
    "#and mlp structure or lstm - lookback of 0 is mlp else use for lstm\n",
    "\n",
    "trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120f19d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864/1864 [==============================] - 1s - loss: 5999.1947 - val_loss: 3570.2073\n"
     ]
    }
   ],
   "source": [
    "#outputs call and train the model with whatever train test data we put in\n",
    "#history used to see our various outputs\n",
    "#data is different if lstm or mlp and we have a lookback\n",
    "\n",
    "LEARNING_RATE = 0.002\n",
    "model, history = run_regression(trainX, trainY, testX, testY, lookback, \n",
    "                                LEARNING_RATE, BATCH_SIZE_LSTM, EPOCHS, layer1_2neurons, dropout, decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTihmZpaL/w8aixtaw5WsdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121530128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864/1864 [==============================] - 0s - loss: 2225958.7564 - val_loss: 4168952.0000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'end_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8e1ea74f31b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mreg_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mrun_set_regressions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-62ae417cf30f>\u001b[0m in \u001b[0;36mrun_set_regressions\u001b[0;34m(params_list)\u001b[0m\n\u001b[1;32m     26\u001b[0m                                         \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lookback'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LEARNING_RATE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BATCH_SIZE_LSTM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                         param['EPOCHS'], layers_to_neurons, param['dropout'])\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mpredY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'end_time' is not defined"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "reg_list = \"reg_list.yml\"\n",
    "\n",
    "\n",
    "with open(\"data/\" + reg_list, 'r') as stream:\n",
    "    try:\n",
    "        regression_dict = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "reg_list = []\n",
    "for reg in regression_dict:\n",
    "    temp_reg = {}\n",
    "    for x in regression_dict[reg]:\n",
    "        for key, value in x.items():\n",
    "            temp_reg[key] = value\n",
    "    reg_list.append(temp_reg)\n",
    "    \n",
    "run_set_regressions(reg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####now we some outputs, performance tests and pretty graphs maybe ?!\n",
    "bugger = model.predict(trainX)\n",
    "plt.plot(bugger)\n",
    "plt.plot(trainY)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugger_test = model.predict(testX)\n",
    "plt.plot(bugger_test)\n",
    "plt.plot(testY)\n",
    "plt.show()\n",
    "\n",
    "len(bugger_test) == len(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'][-1000:])\n",
    "plt.plot(history.history['val_loss'][-1000:])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stuff to look at outputs\n",
    "\n",
    "diff_bugger = np.diff(bugger[:,0])\n",
    "diff_bugger = [1 if x > 0 else 0 for x in diff_bugger]\n",
    "diff_trainY = np.diff(trainY)\n",
    "diff_trainY = [1 if x > 0 else 0 for x in diff_trainY]\n",
    "\n",
    "#confusions matrix on the train set\n",
    "c = confusion_matrix(diff_bugger, diff_trainY, labels=[0,1])\n",
    "print(c)\n",
    "print((c[0,0]+c[1,1])/((c[0,0]+c[1,1]) +(c[0,1]+ c[1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_bugger_test = np.diff(bugger_test[:,0])\n",
    "diff_bugger_test = [1 if x > 0 else 0 for x in diff_bugger_test]\n",
    "diff_testY = np.diff(testY)\n",
    "diff_testY = [1 if x > 0 else 0 for x in diff_testY]\n",
    "\n",
    "\n",
    "#confusion matrix on the val set \n",
    "c = confusion_matrix(diff_bugger_test, diff_testY, labels=[0,1])\n",
    "print(c)\n",
    "print((c[0,0]+c[1,1])/((c[0,0]+c[1,1]) +(c[0,1]+ c[1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding goodness of fit measures\n",
    "# MAPE\n",
    "def compute_mape(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    return np.sum(np.abs((y-y_pred) / y)) / len(y)\n",
    "\n",
    "def compute_r(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    return  np.dot(y - mean_pred, y_pred - mean_pred) / np.sqrt(np.sum(np.square(y - mean_pred)) * np.sum(np.square(y_pred - mean_pred)))\n",
    "\n",
    "def compute_theil_u(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    \n",
    "    n = len(y)\n",
    "    temp = np.sqrt(np.sum(np.square(y)) / n)\n",
    "    temp += np.sqrt(np.sum(np.square(y_pred)) / n)\n",
    "    temp = np.sqrt(np.sum(np.square(y - y_pred)) / n) / temp \n",
    "    return temp\n",
    "\n",
    "predY = bugger_test[:,0]\n",
    "print(compute_mape(testY, predY))\n",
    "print(compute_r(testY, predY))\n",
    "print(compute_theil_u(testY, predY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predY = bugger_test[:,0]\n",
    "print(\"MAPE: {0:.4f}\".format(compute_mape(testY, predY)))\n",
    "print(\"R: {0:.4f}\".format(compute_r(testY, predY)))\n",
    "print(\"THEIL U: {0:.4f}\".format(compute_theil_u(testY, predY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Profitability\n",
    "\n",
    "BUY_COST = 0.25/100\n",
    "SELL_COST = 0.45/100\n",
    "\n",
    "def daily_pnl(y_tomorrow, y_today, buy=True, buy_cost=BUY_COST, sell_cost=SELL_COST):\n",
    "    if(buy):\n",
    "        return (y_tomorrow - y_today - (sell_cost*y_tomorrow + buy_cost*y_today)) / y_today\n",
    "    else:\n",
    "        return (y_today - y_tomorrow - (buy_cost*y_today + sell_cost*y_tomorrow)) / y_today\n",
    "    \n",
    "\n",
    "def buy_and_sell(y_truth, y_pred):\n",
    "    assert len(y_pred) == len(y_truth)\n",
    "    n = len(y_pred)\n",
    "    # Vector of 1s for buy, 0s for sell. Approx: no exact predictions...\n",
    "    buy_sell = [1 if y_pred[index+1] > y_truth[index] else -1 if y_pred[index+1] < y_truth[index] else 0 for index in range(n-1)]\n",
    "    ret = [daily_pnl(y_truth[index+1], y_truth[index]) if buy_sell[index] == 1 else\n",
    "           daily_pnl(y_truth[index+1], y_truth[index], buy=False) for index in range(n-1)]\n",
    "    return np.sum(ret)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_and_sell(bugger_test, testY)\n",
    "\n",
    "#  Change to WT make_raw_data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new features including momentum:\n",
    "\n",
    "x_train_new = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "# Momentum\n",
    "def create_returns(vector, log_returns=False):\n",
    "    if log_returns:\n",
    "        return np.diff(np.log(vector))\n",
    "    else:\n",
    "        return np.diff(vector)\n",
    "    \n",
    "def create_momentum(vector, univariate=True):\n",
    "    ret_d = create_returns(vector, log_returns=True)\n",
    "    ret_d[np.isnan(ret_d)] = 0 #Nan values set to 0. it will correspond to carry over when cumulating\n",
    "    \n",
    "    if univariate:\n",
    "        ret_d = ret_d.reshape(ret_d.shape[0], 1)\n",
    "        \n",
    "    ret_d_sliding = np.zeros([ret_d.shape[0]-19,20,ret_d.shape[1]])\n",
    "\n",
    "    for i in range(0, len(ret_d)-19):\n",
    "        ret_d_sliding[i,:,:] = ret_d[i:i+20,:]\n",
    "\n",
    "    ret_d_sliding = ret_d_sliding[((52*5)):,:,:]\n",
    "    ret_d_sliding = ret_d_sliding[np.arange(0,ret_d_sliding.shape[0],5),:,:]\n",
    "    \n",
    "    cumRet_d = np.cumsum(ret_d_sliding,1)\n",
    "    \n",
    "    mean_d = np.mean(cumRet_d, 1)\n",
    "    \n",
    "    std_d = np.std(cumRet_d, 1)\n",
    "    print(cumRet_d[1].shape)\n",
    "    \n",
    "    d_Zscore = (cumRet_d - np.tile(mean_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))) / np.tile(std_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))\n",
    "    return d_Zscore\n",
    "\n",
    "\n",
    "\n",
    "ok = (create_momentum(x_train_new[\"Close_Price\"]))\n",
    "plt.plot(ok[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily_pr = data[3:,1:]\n",
    "\n",
    "ret_d = np.diff(np.log(daily_pr))\n",
    "ret_d[np.isnan(ret_d)] = 0 #Nan values set to 0. it will correspond to carry over when cumulating\n",
    "ret_w = np.sum(np.reshape(ret_d, (int(ret_d.shape[0]/5),5,ret_d.shape[1])), axis=1)\n",
    "\n",
    "ret_w_sliding = np.zeros([ret_w.shape[0]-51,52,ret_w.shape[1]])\n",
    "ret_d_sliding = np.zeros([ret_d.shape[0]-19,20,ret_d.shape[1]])\n",
    "\n",
    "for i in range(0, len(ret_w)-51):\n",
    "    ret_w_sliding[i,:,:] = ret_w[i:i+52,:]\n",
    "     \n",
    "for i in range(0, len(ret_d)-19):\n",
    "    ret_d_sliding[i,:,:] = ret_d[i:i+20,:]\n",
    "\n",
    "ret_w_sliding = ret_w_sliding[0:-4,:,:]\n",
    "ret_d_sliding = ret_d_sliding[((52*5)):,:,:]\n",
    "ret_d_sliding = ret_d_sliding[np.arange(0,ret_d_sliding.shape[0],5),:,:]\n",
    "\n",
    "cumRet_d = np.cumsum(ret_d_sliding,1)\n",
    "cumRet_w = np.cumsum(ret_w_sliding,1)\n",
    "\n",
    "mean_d = np.mean(cumRet_d,2)\n",
    "mean_w = np.mean(cumRet_w,2)\n",
    "std_d = np.std(cumRet_d,2)\n",
    "std_w = np.std(cumRet_w,2)\n",
    "\n",
    "d_Zscore = (cumRet_d - np.tile(mean_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))) / np.tile(std_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))\n",
    "w_Zscore = (cumRet_w - np.tile(mean_w[:,:,np.newaxis],(1,1,cumRet_w.shape[2]))) / np.tile(std_w[:,:,np.newaxis],(1,1,cumRet_w.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_set_regressions(params_list):\n",
    "    output_list = []   \n",
    "    for param in params_list:\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "        if param['lookback'] != 0:\n",
    "            layers_to_neurons =  param['LSTM_neurons']\n",
    "        else:\n",
    "            layers_to_neurons =  param['MLP_neurons']\n",
    "\n",
    "        #get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "        x_train, y_train = make_raw_data(param['input_file'], param['features'], param['wavelet'], \n",
    "                                         param['normalise'], param['cheat'], param['cheat_fac'])\n",
    "        #then get the output from the autoencoder\n",
    "        if param['auto']:\n",
    "            latent_val = sae(x_train, param['N_EPOCHS'], param['BATCH_SIZE'], \n",
    "                             param['LEARNING_RATE'], param['neurons']) #the output is normalised\n",
    "        else:\n",
    "            latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "            latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "        trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, param['lookback'])   \n",
    "        \n",
    "        \n",
    "        model, history = run_regression(trainX, trainY, testX, testY, \n",
    "                                        param['lookback'], param['LEARNING_RATE'], param['BATCH_SIZE_LSTM'], \n",
    "                                        param['EPOCHS'], layers_to_neurons, param['dropout'])\n",
    "        end_time - start_time - time.time()\n",
    "        predY = model.predict(testX)\n",
    "        \n",
    "        performance_metrics = {'MAPE': compute_mape(testY, predY),\n",
    "                              'R': compute_r(testY, predY),\n",
    "                              'theilU': compute_theil_u(testY, predY),\n",
    "                              'buy_and_sell': buy_and_sell(testY, predY),\n",
    "                               'preparation_and_training_time':end_time,\n",
    "                              'testY': np.array(testY),\n",
    "                              'predY': np.array(predY)}\n",
    "        performance_metrics = np.array(performance_metrics)\n",
    "        \n",
    "        \n",
    "        # Saving everything\n",
    "        model.save(params_list['input_file'] + \".h5\")\n",
    "        np.save(performance_metrics, params_list['input_file'] + \".npy\")\n",
    "        \n",
    "        output_list.append((param, model, history, performance_metrics))\n",
    "    \n",
    "    return output_list\n",
    "         \n",
    "def cross_validate(parameters, list_to_pick_from, name_param):\n",
    "    assert name_param in parameters.keys() # check if name exists\n",
    "    params_list = []\n",
    "    \n",
    "    for hyperparam in list_to_pick_from:\n",
    "        pr = parameters.copy()\n",
    "        pr[name_param] = hyperparam\n",
    "        params_list.append(pr)\n",
    "        \n",
    "    output_list = run_set_regressions(params_list)\n",
    "    \n",
    "    # final validation loss as mean loss value over the last 10 epochs:\n",
    "    val_losses = [np.mean(oupt[2].history['val_loss'][-10:]) for oupt in output_list]\n",
    "    print(\"val_losses\".format(val_losses))\n",
    "    return list_to_pick_from[np.argmin(val_losses)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-af0313ce424b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mplot_loss_vs_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_list' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_loss_vs_epoch(history):\n",
    "    # plot train and validation loss\n",
    "    plt.plot(history.history['loss'][-1000:])\n",
    "    plt.plot(history.history['val_loss'][-1000:])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "for output in output_list:\n",
    "    plot_loss_vs_epoch(output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_file = \"sp500_index_data.csv\"\n",
    "\n",
    "#select which features we are going to examine\n",
    "features = ['Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "           'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "           'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\n",
    "cheat = False\n",
    "cheat_fac = 0.0 \n",
    "normalise = True #normalise data\n",
    "\n",
    "wavelet = False #use wavelet transform\n",
    "auto = False #use autoenconder - if no to both the raw normalised data goes into the lstm or mlp\n",
    "lookback = 4 #if 0 an MLP else the lookback period for an lstm\n",
    "dropout=0.0 #dropout parameter for generalisation tuning\n",
    "\n",
    "#autoencoder settings using relu activation\n",
    "N_EPOCHS=1000\n",
    "neurons = [20,15,15,10] #number of neurons for each layer of the stacked autoencoder\n",
    "BATCH_SIZE = 60\n",
    "LEARNING_RATE = 0.005 \n",
    "\n",
    "#LSTM or MLP Settings\n",
    "BATCH_SIZE_LSTM = 50\n",
    "EPOCHS = 500\n",
    "LSTM_neurons = [50,20]\n",
    "MLP_neurons = [20,10]\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'lookback':lookback,\n",
    "    'LEARNING_RATE':LEARNING_RATE,\n",
    "    'BATCH_SIZE_LSTM':BATCH_SIZE_LSTM,\n",
    "    'EPOCHS':EPOCHS,\n",
    "    'layer1_2neurons':layer1_2neurons,\n",
    "    'dropout':dropout,\n",
    "    'cheat':cheat,\n",
    "    'cheat_fac':cheat_fac,\n",
    "    'wavelet':wavelet,\n",
    "    'auto':auto,\n",
    "    'N_EPOCHS':N_EPOCHS,\n",
    "    'BATCH_SIZE':BATCH_SIZE,\n",
    "    'LSTM_neurons':LSTM_neurons,\n",
    "    'MLP_neurons':MLP_neurons,\n",
    "    'features':features,\n",
    "    'input_file':input_file,\n",
    "    'normalise':normalise\n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# How to use cross validation functions?\n",
    "list_hyper = [0.05, 0.2, 0.005]\n",
    "cross_validate(parameters, list_lr, 'LEARNING_RATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
