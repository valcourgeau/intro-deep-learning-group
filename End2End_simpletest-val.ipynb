{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import backend as K\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wavelet functions\n",
    "# Definining variables for WT\n",
    "# this is the wavelet creation call part...\n",
    "current_level = 2\n",
    "current_mode = 'constant'\n",
    "current_wavelet = pywt.Wavelet('haar')\n",
    "\n",
    "def save_as_csv(data_array, file_name=\"data\"):\n",
    "    output_name = file_name + \".csv\"\n",
    "    np.savetxt(output_name, data_array, delimiter=\",\", fmt='%-7.4f', \n",
    "               header=str(data_array.dtype.names).replace('(',\"\").replace(')',\"\").replace(\"#\", \"\") + \" \",\n",
    "              comments='')\n",
    "    \n",
    "def apply_wt_once(data, wavelet='haar', level=2, mode='constant'):\n",
    "    haar_dwt = pywt.wavedec(data, wavelet=wavelet, level=level, mode=mode)\n",
    "    \n",
    "    # delete the high frequencies from the decomposition\n",
    "    haar_dwt[1] = np.zeros_like(haar_dwt[1])\n",
    "    haar_dwt[2] = np.zeros_like(haar_dwt[2])\n",
    "  \n",
    "    return pywt.waverec(haar_dwt, mode=mode, wavelet=wavelet)\n",
    "\n",
    "def apply_wt_twice(data, wavelet='haar', level=2, mode='constant'):\n",
    "    wt_results = apply_wt_once(data=data,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "    \n",
    "    return apply_wt_once(data=wt_results,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "\n",
    "def get_accuracy(data1, data2):\n",
    "    return np.std(data1-data2)\n",
    "\n",
    "def apply_wt_twice_on_dataset(dataset, wavelet='haar', level=2, mode='constant'):\n",
    "    #ignored_col_names = ('Ntime', 'time', 'Time') # from the SP500 dataset, might need some tweaking\n",
    "    col_names = dataset.dtype.names\n",
    "    results = np.zeros_like(dataset)\n",
    "\n",
    "    #results[ignored_col_names[0]] = dataset[ignored_col_names[0]]\n",
    "    #if ignored_col_names[1] in dataset.dtype.names:\n",
    "    #    results[ignored_col_names[1]] = dataset[ignored_col_names[1]]\n",
    "    #else:\n",
    "    #    results[ignored_col_names[2]] = dataset[col_names[1]]\n",
    "    \n",
    "    for name in col_names:\n",
    "        #if name not in ignored_col_names:\n",
    "        temp = apply_wt_twice(\n",
    "                            dataset[name],\n",
    "                            wavelet=wavelet,\n",
    "                            level=level,\n",
    "                            mode=mode\n",
    "                             )\n",
    "        results[name] = temp[:len(dataset[name])]\n",
    "    return results\n",
    " \n",
    "def end_to_end_twice_wt_with_csv(input_file_name, output_file_name, wavelet='haar', level=2, mode='constant'):\n",
    "    data_array = np.genfromtxt('data/' + input_file_name, delimiter=',', dtype=float, names=True)\n",
    "    res = apply_wt_twice_on_dataset(data_array,\n",
    "                                wavelet=wavelet,\n",
    "                                level=level,\n",
    "                                mode=mode)\n",
    "    save_as_csv(res, file_name=\"data/data_wt/\" + output_file_name)\n",
    "    res = [list(x) for x in res]\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder functions\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                       activation, learning_rate = 0.005, l2_reg = 0.0005,\n",
    "                      seed=42):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            activation=activation,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=None, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                #print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise dataset pre and post autoencoder input\n",
    "def normalise_dataset(dataset):\n",
    "    n_cols = dataset.shape[1]\n",
    "    norm_dataset = np.zeros_like(dataset)\n",
    "    for i in range(n_cols): \n",
    "        mean = np.sum(dataset[:,i])/len(dataset[:,i])\n",
    "        std_dev = np.dot(dataset[:,i]-mean, dataset[:,i]-mean)/(len(dataset[:,i]) - 1)\n",
    "        std_dev = np.sqrt(std_dev)\n",
    "        norm_dataset[:,i] = (dataset[:,i] - mean) / std_dev\n",
    "\n",
    "    return norm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion code to create lookbacks for lstms\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(trainX, trainY, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(trainX)-look_back-1):\n",
    "        a = trainX[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(trainY[i + look_back-1])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the stacked autoencoder\n",
    "#then take the latent layer\n",
    "\n",
    "def sae(x_train, N_EPOCHS, BATCH_SIZE,LEARNING_RATE, neurons):\n",
    "    reset_graph()\n",
    "    ACTIVATION = tf.nn.tanh\n",
    "\n",
    "    hidden_output1, W1, b1, W8, b8 = train_autoencoder(x_train, n_neurons=neurons[0], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output2, W2, b2, W7, b7 = train_autoencoder(hidden_output1, n_neurons=neurons[1], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output3, W3, b3, W6, b6 = train_autoencoder(hidden_output2, n_neurons=neurons[2], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    o4, W4, b4, W5, b5 = train_autoencoder(hidden_output3,n_neurons=neurons[3], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "\n",
    "\n",
    "    #Extract the latent outputs from the stacked ae\n",
    "    reset_graph()\n",
    "    activation = ACTIVATION\n",
    "\n",
    "    n_inputs = 1*(x_train.shape[1])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    \n",
    "    hidden1 = activation(tf.matmul(X, W1) + b1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, W2) + b2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, W3) + b3)\n",
    "    hidden4 = activation(tf.matmul(hidden3, W4) + b4)\n",
    "    hidden5 = activation(tf.matmul(hidden4, W5) + b5)\n",
    "    hidden6 = activation(tf.matmul(hidden5, W6) + b6)\n",
    "    hidden7 = activation(tf.matmul(hidden6, W7) + b7)\n",
    "    \n",
    "    \n",
    "    #hidden1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    #hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2)\n",
    "    #hidden3 = tf.nn.relu(tf.matmul(hidden2, W3) + b3)\n",
    "    #hidden4 = tf.nn.relu(tf.matmul(hidden3, W4) + b4)\n",
    "    #hidden5 = tf.nn.relu(tf.matmul(hidden4, W5) + b5)\n",
    "    #hidden6 = tf.nn.relu(tf.matmul(hidden5, W6) + b6)\n",
    "    #hidden7 = tf.nn.relu(tf.matmul(hidden6, W7) + b7)\n",
    "    outputs = tf.matmul(hidden7, W8) + b8\n",
    "\n",
    "    #unnecessary - should be the same as o4 but just to check\n",
    "    with tf.Session() as sess:\n",
    "            latent_val = hidden4.eval(feed_dict={X: x_train})\n",
    "\n",
    "    # Normalise output of SAEs\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "    # Save output in .csv file\n",
    "    latent_out = pd.DataFrame(latent_val) #latent_val\n",
    "    latent_out.to_csv('data/auto_out.csv', index=False)\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    return latent_val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the moment just a holder function for running the lstm or mlp\n",
    "#can tweak parameters depending on tests\n",
    "\n",
    "def run_regression(trainX, trainY, testX, testY, look_back, LEARNING_RATE, BATCH_SIZE, EPOCHS, neurons, dropout):\n",
    "    #reset_graph()\n",
    "    K.clear_session()\n",
    "    # create and fit the LSTM network\n",
    "    \n",
    "    opti_adam = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    features = trainX.shape[1]\n",
    "    lstm_features = trainX.shape[-1]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #lstm test\n",
    "    if look_back != 0:\n",
    "        for i in enumerate(neurons):\n",
    "            if i[0] != len(neurons)-1:\n",
    "                model.add(LSTM(i[1], input_shape=(look_back, lstm_features),  return_sequences=True, unroll=True))\n",
    "                model.add((Dropout(dropout)))\n",
    "            else:\n",
    "                model.add(LSTM(i[1]))\n",
    "                model.add((Dropout(dropout)))        \n",
    "    #MLP Test\n",
    "    else:\n",
    "        for i in enumerate(neurons): \n",
    "            model.add(Dense(i[1], input_dim=features, activation= 'relu' ))\n",
    "            model.add((Dropout(dropout)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=opti_adam)\n",
    "    history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "              verbose=1, callbacks=[plot_losses], validation_data=(testX, testY)) #validation_split = 0.1)\n",
    "    \n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to create data set inputs to the mlp or lstm\n",
    "def make_reg_data(latent_val, y_train, look_back):\n",
    "\n",
    "    features = latent_val.shape[1]\n",
    "    all_trainX = latent_val \n",
    "\n",
    "    trainX = all_trainX[:round(0.9*len(all_trainX))]\n",
    "    testX = all_trainX[round(0.9*len(all_trainX)):]\n",
    "\n",
    "    all_trainY = np.array(y_train[:-1])\n",
    "    trainY = all_trainY[:round(0.9*len(all_trainY))]\n",
    "    testY = all_trainY[round(0.9*len(all_trainY)):]\n",
    "    \n",
    "    if look_back != 0:\n",
    "        ##### for an mlp dont do the lookback stuff ######\n",
    "        trainX, trainY = create_dataset(trainX, trainY, look_back)\n",
    "        testX, testY = create_dataset(testX, testY, look_back) \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to enable live graph of losses as training progresses \n",
    "#note it is called in this cell also and used later\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Live plot for loss during training\n",
    "# Taken from: https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Reset the plot to only have the last 100 epochs\n",
    "        if len(self.x) % 100 == 0:\n",
    "            self.on_train_begin()\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine initial features\n",
    "def make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac):\n",
    "    #including the creation of the wavelet stuff\n",
    "    #returns normalised X, and target y, currently hard coded files\n",
    "    #for targets it assumes a close price for the input file\n",
    "    #also for now hard coded to data/data_wt\n",
    "    \n",
    "    raw_data = pd.read_csv(\"data/\" + input_file)\n",
    "\n",
    "    #set up with complete cheat features to check it can learn - an MLP does what about lSTM\n",
    "    cheat_data = raw_data[features].copy() # raw_data[['Ntime', 'time', 'Close Price','Open Price']].copy()\n",
    "    if cheat:\n",
    "        cheat_data[\"Cheat\"] = raw_data['Close Price'].shift(-1)\n",
    "    cheat_data = cheat_data[:-1] #to keep it clean will take the last off whether cheat or not - because of y's\n",
    "    #print(cheat_data.head())\n",
    "    cheat_data.to_csv('data/cheat.csv', index=False)\n",
    "    \n",
    "    #run Valentin's wavelet stuff\n",
    "    ok = end_to_end_twice_wt_with_csv(\"cheat.csv\", \"cheat500_wt\")\n",
    "\n",
    "    if wavelet:\n",
    "        x_train = np.genfromtxt('data/data_wt/cheat500_wt.csv', delimiter=',', dtype=None, names=True)\n",
    "    else:\n",
    "        x_train = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "    x_train = [list(item) for item in x_train]\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    #set up target and test y's hard coded target price\n",
    "    y_lazy = pd.read_csv(\"data/\"+input_file)['Close Price'] \n",
    "    y_train = y_lazy.shift(-1)\n",
    "    \n",
    "    #because of the shift the last value will be n/a thus...\n",
    "    x_train = x_train[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "\n",
    "    # Normalised dataset\n",
    "    if normalise:\n",
    "        x_train = normalise_dataset(x_train)\n",
    "        \n",
    "    if cheat:\n",
    "        x_train[:,-1] = x_train[:,-1] +np.random.randn(x_train.shape[0])*cheat_fac\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "####start here - set up our various parameter and design choices ###\n",
    "\n",
    "\n",
    "#the column list available here is ...\n",
    "\"\"\"['Ntime', 'time', 'Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "       'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "       'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\"\"\"\n",
    "\n",
    "input_file = \"sp500_index_data.csv\"\n",
    "\n",
    "#select which features we are going to examine\n",
    "features = ['Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "           'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "           'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\n",
    "cheat = True #do we include future information as an extra feature to prove and tune these things learning\n",
    "#if we do then we can noise it up to make it less of an oracle - this is a multiplier of gaussian noise added to the\n",
    "#last column of training data (where we put our cheat in) - note the data is normalised by this point\n",
    "#0.0 is perfect foresight, as the fac goes up, so does the noise until its a useless feature\n",
    "cheat_fac = 0.0 \n",
    "normalise = True #normalise data\n",
    "\n",
    "wavelet = False #use wavelet transform\n",
    "auto = False #use autoenconder - if no to both the raw normalised data goes into the lstm or mlp\n",
    "lookback = 0 #if 0 an MLP else the lookback period for an lstm\n",
    "dropout=0.0 #dropout parameter for generalisation tuning\n",
    "\n",
    "#autoencoder settings using relu activation\n",
    "N_EPOCHS=1000\n",
    "neurons = [20,15,15,10] #number of neurons for each layer of the stacked autoencoder\n",
    "BATCH_SIZE = 60\n",
    "LEARNING_RATE = 0.005 \n",
    "\n",
    "#LSTM or MLP Settings\n",
    "BATCH_SIZE_LSTM = 50\n",
    "EPOCHS = 500\n",
    "LSTM_neurons = [50,20]\n",
    "MLP_neurons = [20,10]\n",
    "\n",
    "if lookback!=0:\n",
    "    layer1_2neurons =  LSTM_neurons\n",
    "else:\n",
    "    layer1_2neurons =  MLP_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "x_train, y_train = make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac)\n",
    "\n",
    "#then get the output from the autoencoder\n",
    "if auto:\n",
    "    latent_val = sae(x_train, N_EPOCHS, BATCH_SIZE, LEARNING_RATE, neurons) #the output is normalised\n",
    "else:\n",
    "    latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "    \n",
    "    \n",
    "#here we create our final data for input to the lstm or mlp\n",
    "#clearly structure different for both\n",
    "#we pass all latent values it will chop into train/ test\n",
    "#and mlp structure or lstm - lookback of 0 is mlp else use for lstm\n",
    "\n",
    "trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-510dda06db9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m model, history = run_regression(trainX, trainY, testX, testY, lookback, \n\u001b[1;32m----> 6\u001b[1;33m                                 LEARNING_RATE, BATCH_SIZE_LSTM, EPOCHS, layer1_2neurons, dropout)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-6136b36cf5af>\u001b[0m in \u001b[0;36mrun_regression\u001b[1;34m(trainX, trainY, testX, testY, look_back, LEARNING_RATE, BATCH_SIZE, EPOCHS, neurons, dropout)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopti_adam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n\u001b[1;32m---> 35\u001b[1;33m               verbose=1, callbacks=[plot_losses], validation_data=(testX, testY)) #validation_split = 0.1)\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1201\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-a1a885601258>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3304\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_autogen_docstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3305\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3306\u001b[1;33m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3307\u001b[0m     \u001b[1;31m# Deprecated: allow callers to override the hold state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3308\u001b[0m     \u001b[1;31m# by passing hold=True|False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mgca\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mgca\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     \"\"\"\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;31m# More ways of creating axes:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mgca\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[1;31m# no axes found, so create one which spans the figure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1019\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, axisbg, **kwargs)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# this call may differ for non-sep axes, e.g., polar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxisbg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfacecolor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             raise TypeError('Both axisbg and facecolor are not None. '\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_init_axis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[1;34m\"move this out of __init__ because non-separable axes don't use it\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXAxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bottom'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'top'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYAxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\spines.py\u001b[0m in \u001b[0;36mregister_axis\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mcla\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mreset_ticks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    772\u001b[0m         \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminorTicks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminorTicks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lastNumMajorTicks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick\u001b[1;34m(self, major)\u001b[0m\n\u001b[0;32m   1727\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m             \u001b[0mtick_kw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_minor_tick_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1729\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mXTick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtick_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, axes, loc, label, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_tickdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick1line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick1line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick2line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick2line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_gridline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick1line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    418\u001b[0m                           \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tickmarkers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                           \u001b[0mmarkersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m                           markeredgewidth=self._width, zorder=self._zorder)\n\u001b[0m\u001b[0;32m    421\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xaxis_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhich\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tick1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, xdata, ydata, linewidth, linestyle, color, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarkerStyle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_marker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_markevery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mset_marker\u001b[1;34m(self, marker)\u001b[0m\n\u001b[0;32m   1169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \"\"\"\n\u001b[1;32m-> 1171\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_marker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1172\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36mset_marker\u001b[1;34m(self, marker)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m_recache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capstyle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'butt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m_set_tickdown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_tickdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_snap_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mscale\u001b[1;34m(self, sx, sy)\u001b[0m\n\u001b[0;32m   1986\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1987\u001b[0m             np.float_)\n\u001b[1;32m-> 1988\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1989\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f2aaeb7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#outputs call and train the model with whatever train test data we put in\n",
    "#history used to see our various outputs\n",
    "#data is different if lstm or mlp and we have a lookback\n",
    "\n",
    "model, history = run_regression(trainX, trainY, testX, testY, lookback, \n",
    "                                LEARNING_RATE, BATCH_SIZE_LSTM, EPOCHS, layer1_2neurons, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####now we some outputs, performance tests and pretty graphs maybe ?!\n",
    "bugger = model.predict(trainX)\n",
    "plt.plot(bugger)\n",
    "plt.plot(trainY)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugger_test = model.predict(testX)\n",
    "plt.plot(bugger_test)\n",
    "plt.plot(testY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'][-1000:])\n",
    "plt.plot(history.history['val_loss'][-1000:])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stuff to look at outputs\n",
    "\n",
    "diff_bugger = np.diff(bugger[:,0])\n",
    "diff_bugger = [1 if x > 0 else 0 for x in diff_bugger]\n",
    "diff_trainY = np.diff(trainY)\n",
    "diff_trainY = [1 if x > 0 else 0 for x in diff_trainY]\n",
    "\n",
    "#confusions matrix on the train set\n",
    "c = confusion_matrix(diff_bugger, diff_trainY, labels=[0,1])\n",
    "print(c)\n",
    "print((c[0,0]+c[1,1])/((c[0,0]+c[1,1]) +(c[0,1]+ c[1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_bugger_test = np.diff(bugger_test[:,0])\n",
    "diff_bugger_test = [1 if x > 0 else 0 for x in diff_bugger_test]\n",
    "diff_testY = np.diff(testY)\n",
    "diff_testY = [1 if x > 0 else 0 for x in diff_testY]\n",
    "\n",
    "\n",
    "#confusion matrix on the val set \n",
    "c = confusion_matrix(diff_bugger_test, diff_testY, labels=[0,1])\n",
    "print(c)\n",
    "print((c[0,0]+c[1,1])/((c[0,0]+c[1,1]) +(c[0,1]+ c[1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding goodness of fit measures\n",
    "# MAPE\n",
    "def compute_mape(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    return np.sum(np.abs((y-y_pred) / y)) / len(y)\n",
    "\n",
    "# R\n",
    "def compute_r(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    r = np.dot(np.square(y - mean_pred),  np.square(np.square(y_pred - mean_pred)))\n",
    "    r = np.dot(y - mean_pred, y_pred - mean_pred) / np.sqrt(r)\n",
    "    return r\n",
    "\n",
    "# Theil U\n",
    "def compute_theil_u(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    return np.sqrt(np.mean(np.square(y-y_pred))) / (np.sqrt(np.mean(np.square(y))) + np.sqrt(np.mean(np.square(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predY = bugger_test[:,0]\n",
    "print(\"MAPE: {0:.4f}\".format(compute_mape(testY, predY)))\n",
    "print(\"R: {0:.4f}\".format(compute_r(testY, predY)))\n",
    "print(\"THEIL U: {0:.4f}\".format(compute_theil_u(testY, predY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Profitability\n",
    "\n",
    "BUY_COST = 0.25/100\n",
    "SELL_COST = 0.45/100\n",
    "\n",
    "def daily_pnl(y_tomorrow, y_today, buy=True, buy_cost=BUY_COST, sell_cost=SELL_COST):\n",
    "    if(buy):\n",
    "        return (y_tomorrow - y_today - (sell_cost*y_tomorrow + buy_cost*y_today)) / y_today\n",
    "    else:\n",
    "        return (y_today - y_tomorrow - (buy_cost*y_today + sell_cost*y_tomorrow)) / y_today\n",
    "    \n",
    "\n",
    "def buy_and_sell(y_truth, y_pred):\n",
    "    assert len(y_pred) == len(y_truth)\n",
    "    n = len(y_pred)\n",
    "    # Vector of 1s for buy, 0s for sell. Approx: no exact predictions...\n",
    "    buy_sell = [1 if y_pred[index+1] > y_truth[index] else -1 if y_pred[index+1] < y_truth[index] else 0 for index in range(n-1)]\n",
    "    ret = [daily_pnl(y_truth[index+1], y_truth[index]) if buy_sell[index] == 1 else\n",
    "           daily_pnl(y_truth[index+1], y_truth[index], buy=False) for index in range(n-1)]\n",
    "    return np.sum(ret)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy_return' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-22fe61a0eb62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstrategy_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbugger_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#  Change to WT make_raw_data ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'strategy_return' is not defined"
     ]
    }
   ],
   "source": [
    "strategy_return(bugger_test, testY)\n",
    "\n",
    "#  Change to WT make_raw_data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features including momentum:\n",
    "\n",
    "x_train_new = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "# Momentum\n",
    "def create_returns(vector, log_returns=False):\n",
    "    if log_returns:\n",
    "        return np.diff(np.log(vector))\n",
    "    else:\n",
    "        return np.diff(vector)\n",
    "    \n",
    "def create_momentum(vector, univariate=True):\n",
    "    ret_d = create_returns(vector, log_returns=True)\n",
    "    ret_d[np.isnan(ret_d)] = 0 #Nan values set to 0. it will correspond to carry over when cumulating\n",
    "    \n",
    "    if univariate:\n",
    "        ret_d = ret_d.reshape(ret_d.shape[0], 1)\n",
    "        \n",
    "    ret_d_sliding = np.zeros([ret_d.shape[0]-19,20,ret_d.shape[1]])\n",
    "\n",
    "    for i in range(0, len(ret_d)-19):\n",
    "        ret_d_sliding[i,:,:] = ret_d[i:i+20,:]\n",
    "\n",
    "    ret_d_sliding = ret_d_sliding[((52*5)):,:,:]\n",
    "    ret_d_sliding = ret_d_sliding[np.arange(0,ret_d_sliding.shape[0],5),:,:]\n",
    "    \n",
    "    cumRet_d = np.cumsum(ret_d_sliding,1)\n",
    "    \n",
    "    mean_d = np.mean(cumRet_d, 1)\n",
    "    \n",
    "    std_d = np.std(cumRet_d, 1)\n",
    "    print(cumRet_d[1].shape)\n",
    "    \n",
    "    d_Zscore = (cumRet_d - np.tile(mean_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))) / np.tile(std_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))\n",
    "    return d_Zscore\n",
    "\n",
    "\n",
    "\n",
    "ok = (create_momentum(x_train_new[\"Close_Price\"]))\n",
    "plt.plot(ok[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pr = data[3:,1:]\n",
    "\n",
    "ret_d = np.diff(np.log(daily_pr))\n",
    "ret_d[np.isnan(ret_d)] = 0 #Nan values set to 0. it will correspond to carry over when cumulating\n",
    "ret_w = np.sum(np.reshape(ret_d, (int(ret_d.shape[0]/5),5,ret_d.shape[1])), axis=1)\n",
    "\n",
    "ret_w_sliding = np.zeros([ret_w.shape[0]-51,52,ret_w.shape[1]])\n",
    "ret_d_sliding = np.zeros([ret_d.shape[0]-19,20,ret_d.shape[1]])\n",
    "\n",
    "for i in range(0, len(ret_w)-51):\n",
    "    ret_w_sliding[i,:,:] = ret_w[i:i+52,:]\n",
    "     \n",
    "for i in range(0, len(ret_d)-19):\n",
    "    ret_d_sliding[i,:,:] = ret_d[i:i+20,:]\n",
    "\n",
    "ret_w_sliding = ret_w_sliding[0:-4,:,:]\n",
    "ret_d_sliding = ret_d_sliding[((52*5)):,:,:]\n",
    "ret_d_sliding = ret_d_sliding[np.arange(0,ret_d_sliding.shape[0],5),:,:]\n",
    "\n",
    "cumRet_d = np.cumsum(ret_d_sliding,1)\n",
    "cumRet_w = np.cumsum(ret_w_sliding,1)\n",
    "\n",
    "mean_d = np.mean(cumRet_d,2)\n",
    "mean_w = np.mean(cumRet_w,2)\n",
    "std_d = np.std(cumRet_d,2)\n",
    "std_w = np.std(cumRet_w,2)\n",
    "\n",
    "d_Zscore = (cumRet_d - np.tile(mean_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))) / np.tile(std_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))\n",
    "w_Zscore = (cumRet_w - np.tile(mean_w[:,:,np.newaxis],(1,1,cumRet_w.shape[2]))) / np.tile(std_w[:,:,np.newaxis],(1,1,cumRet_w.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_set_regressions(params_list):\n",
    "    output_list = []   \n",
    "    for param in params_list:\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "        if param['lookback'] != 0:\n",
    "            layers_to_neurons =  param['LSTM_neurons']\n",
    "        else:\n",
    "            layers_to_neurons =  param['MLP_neurons']\n",
    "\n",
    "        #get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "        x_train, y_train = make_raw_data(param['input_file'], param['features'], param['wavelet'], \n",
    "                                         param['normalise'], param['cheat'], param['cheat_fac'])\n",
    "        #then get the output from the autoencoder\n",
    "        if param['auto']:\n",
    "            latent_val = sae(x_train, param['N_EPOCHS'], param['BATCH_SIZE'], \n",
    "                             param['LEARNING_RATE'], param['neurons']) #the output is normalised\n",
    "        else:\n",
    "            latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "            latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "\n",
    "        #here we create our final data for input to the lstm or mlp\n",
    "        #clearly structure different for both\n",
    "        #we pass all latent values it will chop into train/ test\n",
    "        #and mlp structure or lstm - lookback of 0 is mlp else use for lstm\n",
    "\n",
    "        trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, param['lookback'])   \n",
    "        \n",
    "        \n",
    "        model, history = run_regression(trainX, trainY, testX, testY, \n",
    "                                        param['lookback'], param['LEARNING_RATE'], param['BATCH_SIZE_LSTM'], \n",
    "                                        param['EPOCHS'], layers_to_neurons, param['dropout'])\n",
    "        end_time - start_time - time.time()\n",
    "        predY = model.predict(testX)\n",
    "        \n",
    "        performance_metrics = {'MAPE': compute_mape(testY, predY),\n",
    "                              'R': compute_r(testY, predY),\n",
    "                              'theilU': compute_theil_u(testY, predY),\n",
    "                              'buy_and_sell': buy_and_sell(testY, predY),\n",
    "                               'preparation_and_training_time':end_time,\n",
    "                              'testY': np.array(testY),\n",
    "                              'predY': np.array(predY)}\n",
    "        \n",
    "        output_list.append((param, model, history, performance_metrics))\n",
    "    \n",
    "    return output_list\n",
    "         \n",
    "def cross_validate(parameters, list_to_pick_from, name_param):\n",
    "    assert name_param in parameters.keys() # check if name exists\n",
    "    params_list = []\n",
    "    \n",
    "    for hyperparam in list_to_pick_from:\n",
    "        pr = parameters.copy()\n",
    "        pr[name_param] = hyperparam\n",
    "        params_list.append(pr)\n",
    "        \n",
    "    output_list = run_set_regressions(params_list)\n",
    "    \n",
    "    # final validation loss as mean loss value over the last 10 epochs:\n",
    "    val_losses = [np.mean(oupt[2].history['val_loss'][-10:]) for oupt in output_list]\n",
    "    print(\"val_losses\".format(val_losses))\n",
    "    return list_to_pick_from[np.argmin(val_losses)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_vs_epoch(history):\n",
    "    # plot train and validation loss\n",
    "    plt.plot(history.history['loss'][-1000:])\n",
    "    plt.plot(history.history['val_loss'][-1000:])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "for output in output_list:\n",
    "    plot_loss_vs_epoch(output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f36357668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864/1864 [==============================] - 0s - loss: 1706602.4418 - val_loss: 3462545.1084\n"
     ]
    }
   ],
   "source": [
    "input_file = \"sp500_index_data.csv\"\n",
    "\n",
    "#select which features we are going to examine\n",
    "features = ['Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "           'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "           'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\n",
    "cheat = False\n",
    "cheat_fac = 0.0 \n",
    "normalise = True #normalise data\n",
    "\n",
    "wavelet = False #use wavelet transform\n",
    "auto = False #use autoenconder - if no to both the raw normalised data goes into the lstm or mlp\n",
    "lookback = 4 #if 0 an MLP else the lookback period for an lstm\n",
    "dropout=0.0 #dropout parameter for generalisation tuning\n",
    "\n",
    "#autoencoder settings using relu activation\n",
    "N_EPOCHS=1000\n",
    "neurons = [20,15,15,10] #number of neurons for each layer of the stacked autoencoder\n",
    "BATCH_SIZE = 60\n",
    "LEARNING_RATE = 0.005 \n",
    "\n",
    "#LSTM or MLP Settings\n",
    "BATCH_SIZE_LSTM = 50\n",
    "EPOCHS = 500\n",
    "LSTM_neurons = [50,20]\n",
    "MLP_neurons = [20,10]\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'lookback':lookback,\n",
    "    'LEARNING_RATE':LEARNING_RATE,\n",
    "    'BATCH_SIZE_LSTM':BATCH_SIZE_LSTM,\n",
    "    'EPOCHS':EPOCHS,\n",
    "    'layer1_2neurons':layer1_2neurons,\n",
    "    'dropout':dropout,\n",
    "    'cheat':cheat,\n",
    "    'cheat_fac':cheat_fac,\n",
    "    'wavelet':wavelet,\n",
    "    'auto':auto,\n",
    "    'N_EPOCHS':N_EPOCHS,\n",
    "    'BATCH_SIZE':BATCH_SIZE,\n",
    "    'LSTM_neurons':LSTM_neurons,\n",
    "    'MLP_neurons':MLP_neurons,\n",
    "    'features':features,\n",
    "    'input_file':input_file,\n",
    "    'normalise':normalise\n",
    "}\n",
    "\n",
    "\n",
    "# How to use cross validation functions?\n",
    "list_hyper = [0.05, 0.2, 0.005]\n",
    "cross_validate(parameters, list_lr, 'LEARNING_RATE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAPE': 182.96436651195972,\n",
       " 'R': array([ 1.50848626]),\n",
       " 'predY': array([[ 203.06254578],\n",
       "        [ 203.06246948],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06254578],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06234741],\n",
       "        [ 203.06248474],\n",
       "        [ 203.06227112],\n",
       "        [ 203.06222534],\n",
       "        [ 203.06234741],\n",
       "        [ 203.06246948],\n",
       "        [ 203.06230164],\n",
       "        [ 203.0612793 ],\n",
       "        [ 203.06240845],\n",
       "        [ 203.06245422],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06211853],\n",
       "        [ 203.06234741],\n",
       "        [ 203.06204224],\n",
       "        [ 203.06182861],\n",
       "        [ 203.06199646],\n",
       "        [ 203.06195068],\n",
       "        [ 203.06173706],\n",
       "        [ 203.06202698],\n",
       "        [ 203.06109619],\n",
       "        [ 203.06169128],\n",
       "        [ 203.06155396],\n",
       "        [ 203.06079102],\n",
       "        [ 203.05979919],\n",
       "        [ 203.05709839],\n",
       "        [ 203.05970764],\n",
       "        [ 203.05726624],\n",
       "        [ 203.05184937],\n",
       "        [ 203.05682373],\n",
       "        [ 202.88565063],\n",
       "        [ 203.01980591],\n",
       "        [ 203.05331421],\n",
       "        [ 203.01992798],\n",
       "        [ 203.03239441],\n",
       "        [ 203.0539093 ],\n",
       "        [ 203.05581665],\n",
       "        [ 203.05882263],\n",
       "        [ 203.05964661],\n",
       "        [ 203.05982971],\n",
       "        [ 203.0561676 ],\n",
       "        [ 203.04708862],\n",
       "        [ 203.00680542],\n",
       "        [ 203.05899048],\n",
       "        [ 203.05987549],\n",
       "        [ 203.05944824],\n",
       "        [ 203.06134033],\n",
       "        [ 203.05677795],\n",
       "        [ 203.06085205],\n",
       "        [ 203.06112671],\n",
       "        [ 203.06089783],\n",
       "        [ 203.05801392],\n",
       "        [ 203.06182861],\n",
       "        [ 203.0617981 ],\n",
       "        [ 203.06066895],\n",
       "        [ 203.06213379],\n",
       "        [ 203.06222534],\n",
       "        [ 203.06243896],\n",
       "        [ 203.06222534],\n",
       "        [ 203.06234741],\n",
       "        [ 203.06246948],\n",
       "        [ 203.06246948],\n",
       "        [ 203.06243896],\n",
       "        [ 203.06240845],\n",
       "        [ 203.06210327],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06248474],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06254578],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06248474],\n",
       "        [ 203.06246948],\n",
       "        [ 203.06245422],\n",
       "        [ 203.06246948],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06254578],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06246948],\n",
       "        [ 203.06242371],\n",
       "        [ 203.06245422],\n",
       "        [ 203.06251526],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06253052],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06251526],\n",
       "        [ 203.06251526],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06243896],\n",
       "        [ 203.06251526],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06091309],\n",
       "        [ 203.0620575 ],\n",
       "        [ 203.06219482],\n",
       "        [ 203.06239319],\n",
       "        [ 203.06246948],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06256104],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06246948],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06253052],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06254578],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06259155],\n",
       "        [ 203.06259155],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06259155],\n",
       "        [ 203.06259155],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06257629],\n",
       "        [ 203.06256104],\n",
       "        [ 203.06259155],\n",
       "        [ 203.06259155],\n",
       "        [ 203.06234741],\n",
       "        [ 203.06248474],\n",
       "        [ 203.06240845],\n",
       "        [ 203.06245422],\n",
       "        [ 203.06251526],\n",
       "        [ 203.06240845],\n",
       "        [ 203.06251526],\n",
       "        [ 203.0625    ],\n",
       "        [ 203.06256104]], dtype=float32),\n",
       " 'testY': array([ 2063.59,  2047.62,  2052.23,  2012.37,  2021.94,  2043.41,\n",
       "         2073.07,  2041.89,  2005.55,  2021.15,  2038.97,  2064.29,\n",
       "         2060.99,  2056.5 ,  2078.36,  2063.36,  2043.94,  2012.66,\n",
       "         2016.71,  1990.26,  1943.09,  1922.03,  1923.67,  1938.68,\n",
       "         1890.28,  1921.84,  1880.33,  1881.33,  1859.33,  1868.99,\n",
       "         1906.9 ,  1877.08,  1903.63,  1882.95,  1893.36,  1940.24,\n",
       "         1939.38,  1903.03,  1912.53,  1915.45,  1880.05,  1853.44,\n",
       "         1852.21,  1851.86,  1829.08,  1864.78,  1895.58,  1926.82,\n",
       "         1917.83,  1917.78,  1945.5 ,  1921.27,  1929.8 ,  1951.7 ,\n",
       "         1948.05,  1932.23,  1978.35,  1986.45,  1993.4 ,  1999.99,\n",
       "         2001.76,  1979.26,  1989.26,  1989.57,  2022.19,  2019.64,\n",
       "         2015.93,  2027.22,  2040.59,  2049.58,  2051.6 ,  2049.8 ,\n",
       "         2036.71,  2035.94,  2037.05,  2055.01,  2063.95,  2059.74,\n",
       "         2072.78,  2066.13,  2045.17,  2066.66,  2041.91,  2047.6 ,\n",
       "         2041.99,  2061.72,  2082.42,  2082.78,  2080.73,  2094.34,\n",
       "         2100.8 ,  2102.4 ,  2091.48,  2091.58,  2087.79,  2091.7 ,\n",
       "         2095.15,  2075.81,  2065.3 ,  2081.43,  2063.37,  2051.12,\n",
       "         2050.63,  2057.14,  2058.69,  2084.39,  2064.46,  2064.11,\n",
       "         2046.61,  2066.66,  2047.21,  2047.63,  2040.04,  2052.32,\n",
       "         2048.04,  2076.06,  2090.54,  2090.1 ,  2099.06,  2096.96,\n",
       "         2099.33,  2105.26,  2099.13,  2109.41,  2112.13,  2119.12,\n",
       "         2115.48,  2096.07,  2079.06,  2075.32,  2071.5 ,  2077.99,\n",
       "         2071.22,  2083.25,  2088.9 ,  2085.45,  2113.32,  2037.41,\n",
       "         2000.54,  2036.09,  2070.77,  2098.86,  2102.95,  2088.55,\n",
       "         2099.73,  2097.9 ,  2129.9 ,  2137.16,  2152.14,  2152.43,\n",
       "         2163.75,  2161.74,  2166.89,  2163.78,  2173.02,  2165.17,\n",
       "         2175.03,  2168.48,  2169.18,  2166.58,  2170.06,  2173.6 ,\n",
       "         2170.84,  2157.03,  2163.79,  2164.25,  2182.87,  2180.89,\n",
       "         2181.74,  2175.49,  2185.79,  2184.05,  2190.15,  2178.15,\n",
       "         2182.22,  2187.02,  2183.87,  2182.64,  2186.9 ,  2175.44,\n",
       "         2172.47,  2169.04,  2180.38,  2176.12,  2170.95,  2170.86,\n",
       "         2179.98,  2186.48,  2186.16,  2181.3 ,  2127.81,  2159.04,\n",
       "         2127.02,  2125.77,  2147.26,  2139.16,  2139.12,  2139.76,\n",
       "         2163.12,  2177.18,  2164.69,  2146.1 ,  2159.93]),\n",
       " 'theilU': 0.82092917463012094}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op[0][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
