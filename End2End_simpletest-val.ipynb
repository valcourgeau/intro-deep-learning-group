{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wavelet functions\n",
    "# Definining variables for WT\n",
    "# this is the wavelet creation call part...\n",
    "current_level = 2\n",
    "current_mode = 'constant'\n",
    "current_wavelet = pywt.Wavelet('haar')\n",
    "\n",
    "def save_as_csv(data_array, file_name=\"data\"):\n",
    "    output_name = file_name + \".csv\"\n",
    "    np.savetxt(output_name, data_array, delimiter=\",\", fmt='%-7.4f', \n",
    "               header=str(data_array.dtype.names).replace('(',\"\").replace(')',\"\").replace(\"#\", \"\") + \" \",\n",
    "              comments='')\n",
    "    \n",
    "def apply_wt_once(data, wavelet='haar', level=2, mode='constant'):\n",
    "    haar_dwt = pywt.wavedec(data, wavelet=wavelet, level=level, mode=mode)\n",
    "    \n",
    "    # delete the high frequencies from the decomposition\n",
    "    haar_dwt[1] = np.zeros_like(haar_dwt[1])\n",
    "    haar_dwt[2] = np.zeros_like(haar_dwt[2])\n",
    "  \n",
    "    return pywt.waverec(haar_dwt, mode=mode, wavelet=wavelet)\n",
    "\n",
    "def apply_wt_twice(data, wavelet='haar', level=2, mode='constant'):\n",
    "    wt_results = apply_wt_once(data=data,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "    \n",
    "    return apply_wt_once(data=wt_results,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "\n",
    "def get_accuracy(data1, data2):\n",
    "    return np.std(data1-data2)\n",
    "\n",
    "def apply_wt_twice_on_dataset(dataset, wavelet='haar', level=2, mode='constant'):\n",
    "    #ignored_col_names = ('Ntime', 'time', 'Time') # from the SP500 dataset, might need some tweaking\n",
    "    col_names = dataset.dtype.names\n",
    "    results = np.zeros_like(dataset)\n",
    "\n",
    "    #results[ignored_col_names[0]] = dataset[ignored_col_names[0]]\n",
    "    #if ignored_col_names[1] in dataset.dtype.names:\n",
    "    #    results[ignored_col_names[1]] = dataset[ignored_col_names[1]]\n",
    "    #else:\n",
    "    #    results[ignored_col_names[2]] = dataset[col_names[1]]\n",
    "    \n",
    "    for name in col_names:\n",
    "        #if name not in ignored_col_names:\n",
    "        temp = apply_wt_twice(\n",
    "                            dataset[name],\n",
    "                            wavelet=wavelet,\n",
    "                            level=level,\n",
    "                            mode=mode\n",
    "                             )\n",
    "        results[name] = temp[:len(dataset[name])]\n",
    "    return results\n",
    " \n",
    "def end_to_end_twice_wt_with_csv(input_file_name, output_file_name, wavelet='haar', level=2, mode='constant'):\n",
    "    data_array = np.genfromtxt('data/' + input_file_name, delimiter=',', dtype=float, names=True)\n",
    "    res = apply_wt_twice_on_dataset(data_array,\n",
    "                                wavelet=wavelet,\n",
    "                                level=level,\n",
    "                                mode=mode)\n",
    "    save_as_csv(res, file_name=\"data/data_wt/\" + output_file_name)\n",
    "    res = [list(x) for x in res]\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoencoder functions\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                       activation, learning_rate = 0.005, l2_reg = 0.0005,\n",
    "                      seed=42):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            activation=activation,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=None, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                #print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise dataset pre and post autoencoder input\n",
    "def normalise_dataset(dataset):\n",
    "    n_cols = dataset.shape[1]\n",
    "    norm_dataset = np.zeros_like(dataset)\n",
    "    for i in range(n_cols): \n",
    "        mean = np.sum(dataset[:,i])/len(dataset[:,i])\n",
    "        std_dev = np.dot(dataset[:,i]-mean, dataset[:,i]-mean)/(len(dataset[:,i]) - 1)\n",
    "        std_dev = np.sqrt(std_dev)\n",
    "        norm_dataset[:,i] = (dataset[:,i] - mean) / std_dev\n",
    "\n",
    "    return norm_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion code to create lookbacks for lstms\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(trainX, trainY, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(trainX)-look_back-1):\n",
    "        a = trainX[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(trainY[i + look_back-1])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the stacked autoencoder\n",
    "#then take the latent layer\n",
    "\n",
    "def sae(x_train, N_EPOCHS, BATCH_SIZE,LEARNING_RATE, neurons):\n",
    "    reset_graph()\n",
    "    ACTIVATION = tf.nn.tanh\n",
    "\n",
    "    hidden_output1, W1, b1, W8, b8 = train_autoencoder(x_train, n_neurons=neurons[0], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output2, W2, b2, W7, b7 = train_autoencoder(hidden_output1, n_neurons=neurons[1], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output3, W3, b3, W6, b6 = train_autoencoder(hidden_output2, n_neurons=neurons[2], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    o4, W4, b4, W5, b5 = train_autoencoder(hidden_output3,n_neurons=neurons[3], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "\n",
    "\n",
    "    #Extract the latent outputs from the stacked ae\n",
    "    reset_graph()\n",
    "    activation = ACTIVATION\n",
    "\n",
    "    n_inputs = 1*(x_train.shape[1])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    \n",
    "    hidden1 = activation(tf.matmul(X, W1) + b1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, W2) + b2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, W3) + b3)\n",
    "    hidden4 = activation(tf.matmul(hidden3, W4) + b4)\n",
    "    hidden5 = activation(tf.matmul(hidden4, W5) + b5)\n",
    "    hidden6 = activation(tf.matmul(hidden5, W6) + b6)\n",
    "    hidden7 = activation(tf.matmul(hidden6, W7) + b7)\n",
    "    \n",
    "    \n",
    "    #hidden1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    #hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2)\n",
    "    #hidden3 = tf.nn.relu(tf.matmul(hidden2, W3) + b3)\n",
    "    #hidden4 = tf.nn.relu(tf.matmul(hidden3, W4) + b4)\n",
    "    #hidden5 = tf.nn.relu(tf.matmul(hidden4, W5) + b5)\n",
    "    #hidden6 = tf.nn.relu(tf.matmul(hidden5, W6) + b6)\n",
    "    #hidden7 = tf.nn.relu(tf.matmul(hidden6, W7) + b7)\n",
    "    outputs = tf.matmul(hidden7, W8) + b8\n",
    "\n",
    "    #unnecessary - should be the same as o4 but just to check\n",
    "    with tf.Session() as sess:\n",
    "            latent_val = hidden4.eval(feed_dict={X: x_train})\n",
    "\n",
    "    # Normalise output of SAEs\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "    # Save output in .csv file\n",
    "    latent_out = pd.DataFrame(latent_val) #latent_val\n",
    "    latent_out.to_csv('data/auto_out.csv', index=False)\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    return latent_val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the moment just a holder function for running the lstm or mlp\n",
    "#can tweak parameters depending on tests\n",
    "\n",
    "def run_regression(trainX, trainY, testX, testY, look_back, LEARNING_RATE, BATCH_SIZE, EPOCHS, neurons, dropout):\n",
    "    #reset_graph()\n",
    "    K.clear_session()\n",
    "    # create and fit the LSTM network\n",
    "    \n",
    "    opti_adam = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    features = trainX.shape[1]\n",
    "    lstm_features = trainX.shape[-1]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #lstm test\n",
    "    if look_back != 0:\n",
    "        for i in enumerate(neurons):\n",
    "            if i[0] != len(neurons)-1:\n",
    "                model.add(LSTM(i[1], input_shape=(look_back, lstm_features),  return_sequences=True, unroll=True))\n",
    "                model.add((Dropout(dropout)))\n",
    "            else:\n",
    "                model.add(LSTM(i[1]))\n",
    "                model.add((Dropout(dropout)))        \n",
    "    #MLP Test\n",
    "    else:\n",
    "        for i in enumerate(neurons): \n",
    "            model.add(Dense(i[1], input_dim=features, activation= 'relu' ))\n",
    "            model.add((Dropout(dropout)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=opti_adam)\n",
    "    history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "              verbose=1, callbacks=[plot_losses], validation_data=(testX, testY)) #validation_split = 0.1)\n",
    "    \n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to create data set inputs to the mlp or lstm\n",
    "def make_reg_data(latent_val, y_train, look_back):\n",
    "\n",
    "    features = latent_val.shape[1]\n",
    "    all_trainX = latent_val \n",
    "\n",
    "    trainX = all_trainX[:round(0.9*len(all_trainX))]\n",
    "    testX = all_trainX[round(0.9*len(all_trainX)):]\n",
    "\n",
    "    all_trainY = np.array(y_train[:-1])\n",
    "    trainY = all_trainY[:round(0.9*len(all_trainY))]\n",
    "    testY = all_trainY[round(0.9*len(all_trainY)):]\n",
    "    \n",
    "    if look_back != 0:\n",
    "        ##### for an mlp dont do the lookback stuff ######\n",
    "        trainX, trainY = create_dataset(trainX, trainY, look_back)\n",
    "        testX, testY = create_dataset(testX, testY, look_back) \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to enable live graph of losses as training progresses \n",
    "#note it is called in this cell also and used later\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Live plot for loss during training\n",
    "# Taken from: https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Reset the plot to only have the last 100 epochs\n",
    "        if len(self.x) % 100 == 0:\n",
    "            self.on_train_begin()\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine initial features\n",
    "def make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac):\n",
    "    #including the creation of the wavelet stuff\n",
    "    #returns normalised X, and target y, currently hard coded files\n",
    "    #for targets it assumes a close price for the input file\n",
    "    #also for now hard coded to data/data_wt\n",
    "    \n",
    "    raw_data = pd.read_csv(\"data/\" + input_file)\n",
    "\n",
    "    #set up with complete cheat features to check it can learn - an MLP does what about lSTM\n",
    "    cheat_data = raw_data[features].copy() # raw_data[['Ntime', 'time', 'Close Price','Open Price']].copy()\n",
    "    if cheat:\n",
    "        cheat_data[\"Cheat\"] = raw_data['Close Price'].shift(-1)\n",
    "    cheat_data = cheat_data[:-1] #to keep it clean will take the last off whether cheat or not - because of y's\n",
    "    #print(cheat_data.head())\n",
    "    cheat_data.to_csv('data/cheat.csv', index=False)\n",
    "    \n",
    "    #run Valentin's wavelet stuff\n",
    "    ok = end_to_end_twice_wt_with_csv(\"cheat.csv\", \"cheat500_wt\")\n",
    "\n",
    "    if wavelet:\n",
    "        x_train = np.genfromtxt('data/data_wt/cheat500_wt.csv', delimiter=',', dtype=None, names=True)\n",
    "    else:\n",
    "        x_train = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "    x_train = [list(item) for item in x_train]\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    #set up target and test y's hard coded target price\n",
    "    y_lazy = pd.read_csv(\"data/\"+input_file)['Close Price'] \n",
    "    y_train = y_lazy.shift(-1)\n",
    "    \n",
    "    #because of the shift the last value will be n/a thus...\n",
    "    x_train = x_train[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "\n",
    "    # Normalised dataset\n",
    "    if normalise:\n",
    "        x_train = normalise_dataset(x_train)\n",
    "        \n",
    "    if cheat:\n",
    "        x_train[:,-1] = x_train[:,-1] +np.random.randn(x_train.shape[0])*cheat_fac\n",
    "    \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "####start here - set up our various parameter and design choices ###\n",
    "\n",
    "\n",
    "#the column list available here is ...\n",
    "\"\"\"['Ntime', 'time', 'Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "       'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "       'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\"\"\"\n",
    "\n",
    "input_file = \"sp500_index_data.csv\"\n",
    "\n",
    "#select which features we are going to examine\n",
    "features = ['Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "           'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "           'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\n",
    "cheat = True #do we include future information as an extra feature to prove and tune these things learning\n",
    "#if we do then we can noise it up to make it less of an oracle - this is a multiplier of gaussian noise added to the\n",
    "#last column of training data (where we put our cheat in) - note the data is normalised by this point\n",
    "#0.0 is perfect foresight, as the fac goes up, so does the noise until its a useless feature\n",
    "cheat_fac = 0.0 \n",
    "normalise = True #normalise data\n",
    "\n",
    "wavelet = False #use wavelet transform\n",
    "auto = False #use autoenconder - if no to both the raw normalised data goes into the lstm or mlp\n",
    "lookback = 0 #if 0 an MLP else the lookback period for an lstm\n",
    "dropout=0.0 #dropout parameter for generalisation tuning\n",
    "\n",
    "#autoencoder settings using relu activation\n",
    "N_EPOCHS=1000\n",
    "neurons = [20,15,15,10] #number of neurons for each layer of the stacked autoencoder\n",
    "BATCH_SIZE = 60\n",
    "LEARNING_RATE = 0.005 \n",
    "\n",
    "#LSTM or MLP Settings\n",
    "BATCH_SIZE_LSTM = 50\n",
    "EPOCHS = 500\n",
    "LSTM_neurons = [50,20]\n",
    "MLP_neurons = [20,10]\n",
    "\n",
    "if lookback!=0:\n",
    "    layer1_2neurons =  LSTM_neurons\n",
    "else:\n",
    "    layer1_2neurons =  MLP_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "x_train, y_train = make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac)\n",
    "\n",
    "#then get the output from the autoencoder\n",
    "if auto:\n",
    "    latent_val = sae(x_train, N_EPOCHS, BATCH_SIZE, LEARNING_RATE, neurons) #the output is normalised\n",
    "else:\n",
    "    latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "    \n",
    "    \n",
    "#here we create our final data for input to the lstm or mlp\n",
    "#clearly structure different for both\n",
    "#we pass all latent values it will chop into train/ test\n",
    "#and mlp structure or lstm - lookback of 0 is mlp else use for lstm\n",
    "\n",
    "trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-510dda06db9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m model, history = run_regression(trainX, trainY, testX, testY, lookback, \n\u001b[1;32m----> 6\u001b[1;33m                                 LEARNING_RATE, BATCH_SIZE_LSTM, EPOCHS, layer1_2neurons, dropout)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-52-6136b36cf5af>\u001b[0m in \u001b[0;36mrun_regression\u001b[1;34m(trainX, trainY, testX, testY, look_back, LEARNING_RATE, BATCH_SIZE, EPOCHS, neurons, dropout)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopti_adam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n\u001b[1;32m---> 35\u001b[1;33m               verbose=1, callbacks=[plot_losses], validation_data=(testX, testY)) #validation_split = 0.1)\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1201\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-a1a885601258>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"val_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3304\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_autogen_docstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3305\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3306\u001b[1;33m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3307\u001b[0m     \u001b[1;31m# Deprecated: allow callers to override the hold state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3308\u001b[0m     \u001b[1;31m# by passing hold=True|False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mgca\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mgca\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     \"\"\"\n\u001b[1;32m--> 950\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgcf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m \u001b[1;31m# More ways of creating axes:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mgca\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[1;31m# no axes found, so create one which spans the figure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1019\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, axisbg, **kwargs)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;31m# this call may differ for non-sep axes, e.g., polar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxisbg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfacecolor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             raise TypeError('Both axisbg and facecolor are not None. '\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_init_axis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[1;34m\"move this out of __init__ because non-separable axes don't use it\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXAxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bottom'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'top'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myaxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYAxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\spines.py\u001b[0m in \u001b[0;36mregister_axis\u001b[1;34m(self, axis)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mcla\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mreset_ticks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    772\u001b[0m         \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminorTicks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 774\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajorTicks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    775\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminorTicks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lastNumMajorTicks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick\u001b[1;34m(self, major)\u001b[0m\n\u001b[0;32m   1727\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m             \u001b[0mtick_kw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_minor_tick_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1729\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mXTick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmajor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtick_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, axes, loc, label, size, width, color, tickdir, pad, labelsize, labelcolor, zorder, gridOn, tick1On, tick2On, label1On, label2On, major)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_tickdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtickdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick1line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick1line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick2line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tick2line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_gridline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick1line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    418\u001b[0m                           \u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'None'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tickmarkers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                           \u001b[0mmarkersize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m                           markeredgewidth=self._width, zorder=self._zorder)\n\u001b[0m\u001b[0;32m    421\u001b[0m         \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xaxis_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhich\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tick1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, xdata, ydata, linewidth, linestyle, color, marker, markersize, markeredgewidth, markeredgecolor, markerfacecolor, markerfacecoloralt, fillstyle, antialiased, dash_capstyle, solid_capstyle, dash_joinstyle, solid_joinstyle, pickradius, drawstyle, markevery, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMarkerStyle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_marker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_markevery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mset_marker\u001b[1;34m(self, marker)\u001b[0m\n\u001b[0;32m   1169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \"\"\"\n\u001b[1;32m-> 1171\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_marker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1172\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36mset_marker\u001b[1;34m(self, marker)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_recache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m_recache\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capstyle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'butt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_marker_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\markers.py\u001b[0m in \u001b[0;36m_set_tickdown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_tickdown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_snap_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\anaconda\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mscale\u001b[1;34m(self, sx, sy)\u001b[0m\n\u001b[0;32m   1986\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1987\u001b[0m             np.float_)\n\u001b[1;32m-> 1988\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1989\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1990\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f2aaeb7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#outputs call and train the model with whatever train test data we put in\n",
    "#history used to see our various outputs\n",
    "#data is different if lstm or mlp and we have a lookback\n",
    "\n",
    "model, history = run_regression(trainX, trainY, testX, testY, lookback, \n",
    "                                LEARNING_RATE, BATCH_SIZE_LSTM, EPOCHS, layer1_2neurons, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####now we some outputs, performance tests and pretty graphs maybe ?!\n",
    "bugger = model.predict(trainX)\n",
    "plt.plot(bugger)\n",
    "plt.plot(trainY)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugger_test = model.predict(testX)\n",
    "plt.plot(bugger_test)\n",
    "plt.plot(testY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'][-1000:])\n",
    "plt.plot(history.history['val_loss'][-1000:])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stuff to look at outputs\n",
    "\n",
    "diff_bugger = np.diff(bugger[:,0])\n",
    "diff_bugger = [1 if x > 0 else 0 for x in diff_bugger]\n",
    "diff_trainY = np.diff(trainY)\n",
    "diff_trainY = [1 if x > 0 else 0 for x in diff_trainY]\n",
    "\n",
    "#confusions matrix on the train set\n",
    "c = confusion_matrix(diff_bugger, diff_trainY, labels=[0,1])\n",
    "print(c)\n",
    "print((c[0,0]+c[1,1])/((c[0,0]+c[1,1]) +(c[0,1]+ c[1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_bugger_test = np.diff(bugger_test[:,0])\n",
    "diff_bugger_test = [1 if x > 0 else 0 for x in diff_bugger_test]\n",
    "diff_testY = np.diff(testY)\n",
    "diff_testY = [1 if x > 0 else 0 for x in diff_testY]\n",
    "\n",
    "\n",
    "#confusion matrix on the val set \n",
    "c = confusion_matrix(diff_bugger_test, diff_testY, labels=[0,1])\n",
    "print(c)\n",
    "print((c[0,0]+c[1,1])/((c[0,0]+c[1,1]) +(c[0,1]+ c[1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adding goodness of fit measures\n",
    "# MAPE\n",
    "def compute_mape(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    return np.sum(np.abs((y-y_pred) / y)) / len(y)\n",
    "\n",
    "# R\n",
    "def compute_r(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    r = np.dot(np.square(y - mean_pred),  np.square(np.square(y_pred - mean_pred)))\n",
    "    r = np.dot(y - mean_pred, y_pred - mean_pred) / np.sqrt(r)\n",
    "    return r\n",
    "\n",
    "# Theil U\n",
    "def compute_theil_u(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    return np.sqrt(np.mean(np.square(y-y_pred))) / (np.sqrt(np.mean(np.square(y))) + np.sqrt(np.mean(np.square(y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predY = bugger_test[:,0]\n",
    "print(\"MAPE: {0:.4f}\".format(compute_mape(testY, predY)))\n",
    "print(\"R: {0:.4f}\".format(compute_r(testY, predY)))\n",
    "print(\"THEIL U: {0:.4f}\".format(compute_theil_u(testY, predY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Profitability\n",
    "\n",
    "BUY_COST = 0.25/100\n",
    "SELL_COST = 0.45/100\n",
    "\n",
    "def daily_pnl(y_tomorrow, y_today, buy=True, buy_cost=BUY_COST, sell_cost=SELL_COST):\n",
    "    if(buy):\n",
    "        return (y_tomorrow - y_today - (sell_cost*y_tomorrow + buy_cost*y_today)) / y_today\n",
    "    else:\n",
    "        return (y_today - y_tomorrow - (buy_cost*y_today + sell_cost*y_tomorrow)) / y_today\n",
    "    \n",
    "\n",
    "def strategy_return(y_pred, y_truth):\n",
    "    assert len(y_pred) == len(y_truth)\n",
    "    n = len(y_pred)\n",
    "    # Vector of 1s for buy, 0s for sell. Approx: no exact predictions...\n",
    "    buy_sell = [1 if y_pred[index+1] > y_truth[index] else -1 if y_pred[index+1] < y_truth[index] else 0 for index in range(n-1)]\n",
    "    ret = [daily_pnl(y_truth[index+1], y_truth[index]) if buy_sell[index] == 1 else\n",
    "           daily_pnl(y_truth[index+1], y_truth[index], buy=False) for index in range(n-1)]\n",
    "    return np.sum(ret)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy_return' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-22fe61a0eb62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstrategy_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbugger_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#  Change to WT make_raw_data ?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'strategy_return' is not defined"
     ]
    }
   ],
   "source": [
    "strategy_return(bugger_test, testY)\n",
    "\n",
    "#  Change to WT make_raw_data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features including momentum:\n",
    "\n",
    "x_train_new = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "# Momentum\n",
    "def create_returns(vector, log_returns=False):\n",
    "    if log_returns:\n",
    "        return np.diff(np.log(vector))\n",
    "    else:\n",
    "        return np.diff(vector)\n",
    "    \n",
    "def create_momentum(vector, univariate=True):\n",
    "    ret_d = create_returns(vector, log_returns=True)\n",
    "    ret_d[np.isnan(ret_d)] = 0 #Nan values set to 0. it will correspond to carry over when cumulating\n",
    "    \n",
    "    if univariate:\n",
    "        ret_d = ret_d.reshape(ret_d.shape[0], 1)\n",
    "        \n",
    "    ret_d_sliding = np.zeros([ret_d.shape[0]-19,20,ret_d.shape[1]])\n",
    "\n",
    "    for i in range(0, len(ret_d)-19):\n",
    "        ret_d_sliding[i,:,:] = ret_d[i:i+20,:]\n",
    "\n",
    "    ret_d_sliding = ret_d_sliding[((52*5)):,:,:]\n",
    "    ret_d_sliding = ret_d_sliding[np.arange(0,ret_d_sliding.shape[0],5),:,:]\n",
    "    \n",
    "    cumRet_d = np.cumsum(ret_d_sliding,1)\n",
    "    \n",
    "    mean_d = np.mean(cumRet_d, 1)\n",
    "    \n",
    "    std_d = np.std(cumRet_d, 1)\n",
    "    print(cumRet_d[1].shape)\n",
    "    \n",
    "    d_Zscore = (cumRet_d - np.tile(mean_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))) / np.tile(std_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))\n",
    "    return d_Zscore\n",
    "\n",
    "\n",
    "\n",
    "ok = (create_momentum(x_train_new[\"Close_Price\"]))\n",
    "plt.plot(ok[:,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pr = data[3:,1:]\n",
    "\n",
    "ret_d = np.diff(np.log(daily_pr))\n",
    "ret_d[np.isnan(ret_d)] = 0 #Nan values set to 0. it will correspond to carry over when cumulating\n",
    "ret_w = np.sum(np.reshape(ret_d, (int(ret_d.shape[0]/5),5,ret_d.shape[1])), axis=1)\n",
    "\n",
    "ret_w_sliding = np.zeros([ret_w.shape[0]-51,52,ret_w.shape[1]])\n",
    "ret_d_sliding = np.zeros([ret_d.shape[0]-19,20,ret_d.shape[1]])\n",
    "\n",
    "for i in range(0, len(ret_w)-51):\n",
    "    ret_w_sliding[i,:,:] = ret_w[i:i+52,:]\n",
    "     \n",
    "for i in range(0, len(ret_d)-19):\n",
    "    ret_d_sliding[i,:,:] = ret_d[i:i+20,:]\n",
    "\n",
    "ret_w_sliding = ret_w_sliding[0:-4,:,:]\n",
    "ret_d_sliding = ret_d_sliding[((52*5)):,:,:]\n",
    "ret_d_sliding = ret_d_sliding[np.arange(0,ret_d_sliding.shape[0],5),:,:]\n",
    "\n",
    "cumRet_d = np.cumsum(ret_d_sliding,1)\n",
    "cumRet_w = np.cumsum(ret_w_sliding,1)\n",
    "\n",
    "mean_d = np.mean(cumRet_d,2)\n",
    "mean_w = np.mean(cumRet_w,2)\n",
    "std_d = np.std(cumRet_d,2)\n",
    "std_w = np.std(cumRet_w,2)\n",
    "\n",
    "d_Zscore = (cumRet_d - np.tile(mean_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))) / np.tile(std_d[:,:,np.newaxis],(1,1,cumRet_d.shape[2]))\n",
    "w_Zscore = (cumRet_w - np.tile(mean_w[:,:,np.newaxis],(1,1,cumRet_w.shape[2]))) / np.tile(std_w[:,:,np.newaxis],(1,1,cumRet_w.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_set_regressions(params_list):\n",
    "    output_list = []   \n",
    "    for param in params_list:\n",
    "        if param['lookback'] != 0:\n",
    "            layers_to_neurons =  param['LSTM_neurons']\n",
    "        else:\n",
    "            layers_to_neurons =  param['MLP_neurons']\n",
    "\n",
    "        #get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "        x_train, y_train = make_raw_data(param['input_file'], param['features'], param['wavelet'], \n",
    "                                         param['normalise'], param['cheat'], param['cheat_fac'])\n",
    "        #then get the output from the autoencoder\n",
    "        if param['auto']:\n",
    "            latent_val = sae(x_train, param['N_EPOCHS'], param['BATCH_SIZE'], \n",
    "                             param['LEARNING_RATE'], param['neurons']) #the output is normalised\n",
    "        else:\n",
    "            latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "            latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "\n",
    "        #here we create our final data for input to the lstm or mlp\n",
    "        #clearly structure different for both\n",
    "        #we pass all latent values it will chop into train/ test\n",
    "        #and mlp structure or lstm - lookback of 0 is mlp else use for lstm\n",
    "\n",
    "        trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, param['lookback'])   \n",
    "\n",
    "        model, history = run_regression(trainX, trainY, testX, testY, \n",
    "                                        param['lookback'], param['LEARNING_RATE'], param['BATCH_SIZE_LSTM'], \n",
    "                                        param['EPOCHS'], layers_to_neurons, param['dropout'])\n",
    "        output_list.append((param, model, history))\n",
    "    \n",
    "    return output_list\n",
    "         \n",
    "def cross_validate(parameters, list_to_pick_from, name_param):\n",
    "    assert name_param in parameters.keys() # check if name exists\n",
    "    params_list = []\n",
    "    \n",
    "    for hyperparam in list_to_pick_from:\n",
    "        pr = parameters.copy()\n",
    "        pr[name_param] = hyperparam\n",
    "        params_list.append(pr)\n",
    "        \n",
    "    output_list = run_set_regressions(params_list)\n",
    "    \n",
    "    # final validation loss as mean loss value over the last 10 epochs:\n",
    "    val_losses = [np.mean(oupt[2].history['val_loss'][-10:]) for oupt in output_list]\n",
    "    print(\"val_losses\".format(val_losses))\n",
    "    return list_to_pick_from[np.argmin(val_losses)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_vs_epoch(history):\n",
    "    # plot train and validation loss\n",
    "    plt.plot(history.history['loss'][-1000:])\n",
    "    plt.plot(history.history['val_loss'][-1000:])\n",
    "    plt.title('model train vs validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "for output in output_list:\n",
    "    plot_loss_vs_epoch(output[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt4VfWd7/H3N/dAroQQAkEBQfFC1WNqbTt2Wn2KjLXV\ndlqxrZW2Pjqn+qi9nKnacUantTO1PWPndC5aZ+rxMnYoo87UGbU82mJtz3gLCnIVUAETQsg9BEgg\nyff8sX47eydAEmDB3sDn9Tzr2Su/vdbKb69WPvld9m+ZuyMiInK4stJdAREROT4oUEREJBYKFBER\niYUCRUREYqFAERGRWChQREQkFgoUERGJhQJFRERioUAREZFY5KS7AkfTxIkTffr06emuhojIMWXZ\nsmUt7l452nEnVKBMnz6durq6dFdDROSYYmabx3KcurxERCQWChQREYmFAkVERGJxQo2hiMiJZ+/e\nvdTX19PT05PuqmS8goICampqyM3NPaTzFSgiclyrr6+nuLiY6dOnY2bprk7GcndaW1upr69nxowZ\nh3QNdXmJyHGtp6eHiooKhckozIyKiorDaskpUETkuKcwGZvDvU/q8hqLFYtgdwdM/R8weS7kFqa7\nRiIiGUeBMharnoQNS6L9rByYdAZMPS8KmKnnQeUcyMpObx1FJGMVFRXR3d2d7moccQqUsfjiYuja\nCg2vQ8My2Pp6FDLL/m/0fu64qOVSdRZMPguq5kLVGZA3Pr31FhE5ihQoY1UyJdpOvyz6eWAA2t6J\nwqVhGTSugJX/BnU/CycYTJiZEjBnRlvZSaD+XJETkrvz7W9/m2effRYz44477mDBggU0NjayYMEC\nurq66Ovr47777uNDH/oQ1157LXV1dZgZX/3qV/nGN76R7o8wIgXKocrKgomzou19V0Zl7tCxGZpW\nw7ZV0LQSGt+ENb9MnpdXHLVeqs6Mus6qzop+LihNz+cQOYH85X+uZs3WrlivecaUEu785JljOvbJ\nJ59k+fLlrFixgpaWFt7//vfzkY98hJ///Odccskl/Nmf/Rn9/f3s2rWL5cuX09DQwKpVqwDo6OiI\ntd5HggIlTmZQPj3a5nwiWd67A7avjYKmaTVsXwOrnoCeB5PHlNREwTIpETanw8RTISf/aH8KETlC\nfv/73/P5z3+e7Oxsqqqq+MM//ENee+013v/+9/PVr36VvXv3csUVV3DOOecwc+ZM3nnnHW666SY+\n8YlPMG/evHRXf1QKlKMhvximnR9tCe7Q1QBNa6BpVRQyTWvg7aUwsDc6JisHKmZF4TIphMyk06PA\n0iQAkYM21pbE0faRj3yEF198kaeffpovf/nLfPOb3+Saa65hxYoVLFmyhPvvv5/Fixfz4IMPjn6x\nNFKgpIsZlNZE26kpf3n074XWjcmWTNOaaIxm9b8nj8kpgMrTotZM5ZzoddLp0bU0PiOSsS688EJ+\n+tOfsnDhQtra2njxxRf50Y9+xObNm6mpqeG6666jt7eX119/nUsvvZS8vDz++I//mNNOO42rr746\n3dUflQIl02TnJlsiqXq7ofmtKGS2r4XmtfDOC7DiX5PH5BWHoDk9uVWeDsWTFTQiGeDTn/40L730\nEmeffTZmxg9/+EMmT57Mww8/zI9+9CNyc3MpKirikUceoaGhga985SsMDAwA8Nd//ddprv3ozN3T\nXYejpra21o+7B2ztbo8CZvtaaF4X9tfArtbkMQVlIVzmDH0dX6mgkePe2rVrOf3000c/UID93y8z\nW+butaOdqxbKsa6wHE7+ULSl6m6OWjGJgGl+K+o2S3x3BqBwQjJgKufApDlRi6Zo1Cd9iojsQ4Fy\nvCqqjLYZH0mWuUN3U7I107wOtq+DVY9DT2fyuHEVUbBMmpMSNqfD+IlH/3OIyDFDgXIiMYvGU4on\nwykfS5a7w45toUWzLhk2b/4b9O4naCpPC0FzmrrORGSQAkWiMCipjrZTLkqWu8OOxtCieSsEzVv7\ntmgKy5MBk/paXK2gETmBKFDkwMySS87MujhZnug6SwRMoutszVOw+6HkcfklIVxOCy2bEDaa3ixy\nXFKgyMFL7Tqb+dFkuTvsbEl2mSXCZv0SeONfksflFUWrAAxv1ZSdHC1pIyLHpDEHipllA3VAg7tf\nZmYTgF8A04FNwJXu3h6OvR24FugHbnb3JaH8POAhoBB4BrjF3d3M8oFHgPOAVmCBu28K5ywE7gjV\nuNvdHw7lM4BFQAWwDPiSu+85pLsg8TBLmQxw4dD3drZCy1tDg+adpbDi58ljcgph4uyUVs0cmHga\nTJgRfT9HRDLawbRQbgHWAiXh59uAX7v7D8zstvDzrWZ2BnAVcCYwBXjezE51937gPuA64BWiQJkP\nPEsUPu3uPsvMrgLuARaE0LoTqAUcWGZmT4Xgugf4sbsvMrP7wzXuO+Q7IUfW+AoYv5/pzbs7oGV9\nNE7Tsj4Kmi0vRys3J2TlQsUpUchMPC0ZOBWzIbfg6H4OkaNgpOenbNq0icsuu2xw0chMMqZAMbMa\n4BPA94FvhuLLgY+G/YeBF4BbQ/kid+8F3jWzjcD5ZrYJKHH3l8M1HwGuIAqUy4G7wrUeB/7eomdR\nXgI85+5t4ZzngPlmtgi4CPhCyu+/CwXKsaewbN91ziBaGaBlfdSaaXkret22Ctb+J3j0zWEsK+om\nq5wDlacmWzSVp0brp4nIUTXWFsrfAt8GUv8rrXL3xrC/DagK+1OBl1OOqw9le8P+8PLEOe8BuHuf\nmXUSdWUNlg87pwLocPe+/VxLjgf5ReGJmP9jaPneHmh7O3SdhRZNy3rY+HxyUU2Akqkp3WYp4zXj\nJhzdzyGZ5dnbYNvKeK85eS780Q9GPOS2225j2rRp3HjjjQDcdddd5OTksHTpUtrb29m7dy933303\nl19++UH96p6eHr72ta9RV1dHTk4O9957Lx/72MdYvXo1X/nKV9izZw8DAwM88cQTTJkyhSuvvJL6\n+nr6+/v58z//cxYsWHDIH3t/Rg0UM7sM2O7uy8zso/s7JoyDZOQaLmZ2PXA9wEknnZTm2shhyy1I\nPqwsVX8ftG8KARNaNM1vwbKHYO+u5HHjJoaus1OTLZuJp0Uz2TTzTI6QBQsW8PWvf30wUBYvXsyS\nJUu4+eabKSkpoaWlhQsuuIBPfepT2EH8//Af/uEfMDNWrlzJunXrmDdvHuvXr+f+++/nlltu4Ytf\n/CJ79uyhv7+fZ555hilTpvD0008D0NnZOcrVD95YWigfBj5lZpcCBUCJmf0L0GRm1e7eaGbVwPZw\nfAMwLeX8mlDWEPaHl6eeU29mOUAp0eB8A8lutcQ5L4T3yswsJ7RSUq81hLs/ADwA0VpeY/i8cizK\nzkk+8IzLkuUDA9D5XrL7LNGiWf3v0JPywKK84uSEgImnJsdryqdH15bjwygtiSPl3HPPZfv27Wzd\nupXm5mbKy8uZPHky3/jGN3jxxRfJysqioaGBpqYmJk+ePObr/v73v+emm24CYM6cOZx88smsX7+e\nD37wg3z/+9+nvr6ez3zmM8yePZu5c+fyrW99i1tvvZXLLruMCy+8cJSrH7xR/0tx99uB2wFCC+V/\nufvVZvYjYCHwg/CaeCzhU8DPzexeokH52cCr7t5vZl1mdgHRoPw1wN+lnLMQeAn4LPCb0OpZAvyV\nmZWH4+YBt4f3loZjFw37/SJJWVlQfnK0zf54stwddjYPDZnmt/ZdwTk7L3omzcRTU4Lm1Ch8cguP\n+seRY9fnPvc5Hn/8cbZt28aCBQt47LHHaG5uZtmyZeTm5jJ9+nR6enpi+V1f+MIX+MAHPsDTTz/N\npZdeyk9/+lMuuugiXn/9dZ555hnuuOMOLr74Yv7iL/4ilt+XcDh/ev0AWGxm1wKbgSsB3H21mS0G\n1gB9wI1hhhfADSSnDT8bNoCfAY+GAfw2olliuHubmX0PeC0c993EAD3RBIBFZnY38Ea4hsjYmEHR\npGgbPsW5pxNaNqRMCFgP296EtU8lJwRgUHZSSsCkhI3GaWQ/FixYwHXXXUdLSwu//e1vWbx4MZMm\nTSI3N5elS5eyefPmg77mhRdeyGOPPcZFF13E+vXr2bJlC6eddhrvvPMOM2fO5Oabb2bLli28+eab\nzJkzhwkTJnD11VdTVlbGP//zP8f+GQ8qUNz9BaIuJ9y9Fbj4AMd9n2hG2PDyOuCs/ZT3AJ87wLUe\nBPZ5TJm7vwOcv+8ZIoepoBRqaqMt1eCEgLdSZqCth3d+C/29yePGVyZnm6W+apzmhHbmmWeyY8cO\npk6dSnV1NV/84hf55Cc/ydy5c6mtrWXOnDkHfc0bbriBr33ta8ydO5ecnBweeugh8vPzWbx4MY8+\n+ii5ublMnjyZ73znO7z22mv86Z/+KVlZWeTm5nLfffFPitXzUEQO10A/dGyOWjItibAJ+6lrnuUV\nRV1liS6ziaFFM2Em5OSlr/7HOT0P5eDoeSgi6ZSVHYXChJlw2vxkuTt0b0/OOmvdGL1u+n/w5i+S\nx1l2tBpAoutssPtsdtRaEjlGKFBEjhQzKK6KttTn0kD0xc3WDaElk2jZbIANzw39Pk1R1b4hM/HU\n6Hs26j47rq1cuZIvfelLQ8ry8/N55ZVX0lSj0SlQRNIhvwimnBttqfr7QvdZ6Dpr2RCFzcrHhz6b\nJnd8SvfZqcn9ilMgJ//ofpZjgLsf1Pc7MsHcuXNZvnz5Uf2dhzsEokARySTZOVEoVJwCXJosT0xz\nHpwMsCHa3/IyrFycPC6xHE0iZFJnoZ2gs88KCgpobW2loqLimAuVo8ndaW1tpaDg0NfHU6CIHAtS\npzlP/4Oh7+3ZGY3PJEIm0bJ554Whs8/GVQxtzST2y06OxoGOUzU1NdTX19Pc3JzuqmS8goICampq\nRj/wADTLS+R4NdAfVgkI36lp3ZAMnZ0p/7hm58GEU/bThTZbi2wKoFleIpKVHS0dUz596CoBALva\nQqsmpQtt+xpY9zQMfg+Z6DHOw4OmYnY0KUAPQ5NhFCgiJ6JxE2Dcfh4b0LcH2t9N6T4Lr2/+27BJ\nAeOGLkkzMexXzNKSNCcwBYqIJOXkJR9glip1UkBq0NS/CqueIHr+HURL0kyLWjGpXWcTT42mQGtQ\n/LimQBGR0Y04KWBX1H3WugFaNiZDZ8tLQx8dkF+S0qpJad1MmKmpzscJBYqIHJ68cVD9vmhLNTAA\nO7amtGg2RKGz6Xfw5qLkcYNTncP4TKJVUzE7CjC1ao4ZChQROTKysqC0JtpOuWjoe73doVWzMRk4\nrRvg3d9B3+7kcfmlUWumYnbKOM3sqFWTe+jfl5AjQ4EiIkdffhFMOSfaUg0MQFdDyhTnA7RqEo8P\nGFwhYFZo1cyKZqapVZMWChQRyRxZWdGgftm0fVs1Q77AuSEZOpv/e+hYTV5RWG1gdjJkElt+0dH9\nPCcYBYqIHBvyxkP12dGWyh26tiYDJhE6+8xAI/r+zGBrZnayO610mr5XEwMFiogc28ygdGq0zfzo\n0Pf27oa2d1JaNGE22vDv1eQUhNUCEq2ZlNZNYdnR/DTHNAWKiBy/cguh6sxoSzX4vZqUrrPWt6Fp\nNaz9r6GrBYyvjAKm4pShM9HKp0N27lH9OJlOgSIiJ54h36v58ND3+vdC+6Zk91miZbP+V/DGo8nj\nsnKiUEmMzwyO15y4050VKCIiqbJzk9+FGW53R8p055RutHdegL6e5HH5JcmJARWzkmM1FadEY0HH\nKQWKiMhYFZZBTW20pRoYgK76lFZNCJwtLw19Xg0kJwYMnxxQOu2Yf4yAAkVE5HBlZUXfiyk7CWZd\nPPS9Pbug7e0QMhuT3WjDn8KZnR99YbPilH270cZVHBNdaAoUEZEjKW8cTJ4bbakSEwMGu882RhMD\nWtbD+iUwsDd5bEHZ0O/TJCYITJiZUV1oChQRkXRInRhw8oeGvtffB51bhrZoWt/ez4oBhC60lFZN\nYqym7OTokdJHkQJFRCTTZOdErY8JM4F5Q9/bszP6bs3gWE0InFVPQE9KF1pWDpTPSE4KuOBGKKk+\notVWoIiIHEvyxh+4Cy3xJM7BVk3oRnv7N3D+9Ue8agoUEZHjgRmMr4i2kz4w9L2BgaMyqK9AERE5\n3h2ldcq0GpqIiMRCgSIiIrFQoIiISCwUKCIiEotRA8XMCszsVTNbYWarzewvQ/ldZtZgZsvDdmnK\nObeb2UYze8vMLkkpP8/MVob3fmIWTTsws3wz+0Uof8XMpqecs9DMNoRtYUr5jHDsxnBuXjy3RERE\nDsVYWii9wEXufjZwDjDfzC4I7/3Y3c8J2zMAZnYGcBVwJjAf+EczS6x4dh9wHTA7bPND+bVAu7vP\nAn4M3BOuNQG4E/gAcD5wp5mVh3PuCb9/FtAeriEiImkyaqB4pDv8mBs2H+GUy4FF7t7r7u8CG4Hz\nzawaKHH3l93dgUeAK1LOeTjsPw5cHFovlwDPuXubu7cDzxEFmgEXhWMJ5yauJSIiaTCmMRQzyzaz\n5cB2on/gXwlv3WRmb5rZgykth6nAeymn14eyqWF/ePmQc9y9D+gEKka4VgXQEY4dfi0REUmDMQWK\nu/e7+zlADVFr4yyi7quZRN1gjcDfHLFaHgYzu97M6sysrrm5Od3VERE5bh3ULC937wCWAvPdvSkE\nzQDwT0RjHAANwLSU02pCWUPYH14+5BwzywFKgdYRrtUKlIVjh19reJ0fcPdad6+trKw8mI8rIiIH\nYSyzvCrNrCzsFwIfB9aFMZGETwOrwv5TwFVh5tYMosH3V929EegyswvCGMg1wC9TzknM4Pos8Jsw\nzrIEmGdm5aFLbR6wJLy3NBxLODdxLRERSYOxrOVVDTwcZmplAYvd/b/M7FEzO4dogH4T8CcA7r7a\nzBYDa4A+4EZ37w/XugF4CCgEng0bwM+AR81sI9BGNEsMd28zs+8Br4XjvuvubWH/VmCRmd0NvBGu\nISIiaWLRH/snhtraWq+rq0t3NUREjilmtszda0c7Tt+UFxGRWChQREQkFgoUERGJhQJFRERioUAR\nEZFYKFBERCQWChQREYmFAkVERGKhQBERkVgoUEREJBYKFBERiYUCRUREYqFAERGRWChQREQkFgoU\nERGJhQJFRERioUAREZFYKFBERCQWChQREYmFAkVERGKhQBERkVgoUEREJBYKFBERiYUCRUREYqFA\nERGRWChQREQkFgoUERGJhQJFRERioUAREZFYKFBERCQWChQREYmFAkVERGIxaqCYWYGZvWpmK8xs\ntZn9ZSifYGbPmdmG8Fqecs7tZrbRzN4ys0tSys8zs5XhvZ+YmYXyfDP7RSh/xcymp5yzMPyODWa2\nMKV8Rjh2Yzg3L55bIiIih2IsLZRe4CJ3Pxs4B5hvZhcAtwG/dvfZwK/Dz5jZGcBVwJnAfOAfzSw7\nXOs+4Dpgdtjmh/JrgXZ3nwX8GLgnXGsCcCfwAeB84M6U4LoH+HE4pz1cQ0RE0mTUQPFId/gxN2wO\nXA48HMofBq4I+5cDi9y9193fBTYC55tZNVDi7i+7uwOPDDsnca3HgYtD6+US4Dl3b3P3duA5okAz\n4KJw7PDfLyIiaTCmMRQzyzaz5cB2on/gXwGq3L0xHLINqAr7U4H3Uk6vD2VTw/7w8iHnuHsf0AlU\njHCtCqAjHDv8WiIikgZjChR373f3c4AaotbGWcPed6JWS8Yxs+vNrM7M6pqbm9NdHRGR49ZBzfJy\n9w5gKdHYR1PoxiK8bg+HNQDTUk6rCWUNYX94+ZBzzCwHKAVaR7hWK1AWjh1+reF1fsDda929trKy\n8mA+roiIHISxzPKqNLOysF8IfBxYBzwFJGZdLQR+GfafAq4KM7dmEA2+vxq6x7rM7IIwBnLNsHMS\n1/os8JvQ6lkCzDOz8jAYPw9YEt5bGo4d/vtFRCQNckY/hGrg4TBTKwtY7O7/ZWYvAYvN7FpgM3Al\ngLuvNrPFwBqgD7jR3fvDtW4AHgIKgWfDBvAz4FEz2wi0Ec0Sw93bzOx7wGvhuO+6e1vYvxVYZGZ3\nA2+Ea4iISJpY9Mf+iaG2ttbr6urSXQ0RkWOKmS1z99rRjtM35UVEJBYKFBERiYUCRUREYqFAERGR\nWChQREQkFgoUERGJhQJFRERioUAREZFYKFBERCQWChQREYmFAkVERGKhQBERkVgoUEREJBYKFBER\niYUCRUREYqFAERGRWChQREQkFgoUERGJhQJFRERioUAREZFYKFBERCQWChQREYmFAkVERGKhQBER\nkVgoUEREJBYKFBERiYUCRUREYqFAERGRWChQREQkFgoUERGJhQJFRERioUAREZFYjBooZjbNzJaa\n2RozW21mt4Tyu8yswcyWh+3SlHNuN7ONZvaWmV2SUn6ema0M7/3EzCyU55vZL0L5K2Y2PeWchWa2\nIWwLU8pnhGM3hnPz4rklIiJyKMbSQukDvuXuZwAXADea2RnhvR+7+zlhewYgvHcVcCYwH/hHM8sO\nx98HXAfMDtv8UH4t0O7us4AfA/eEa00A7gQ+AJwP3Glm5eGce8LvnwW0h2uIiEiajBoo7t7o7q+H\n/R3AWmDqCKdcDixy9153fxfYCJxvZtVAibu/7O4OPAJckXLOw2H/ceDi0Hq5BHjO3dvcvR14Dpgf\n3rsoHEs4N3EtERFJg4MaQwldUecCr4Sim8zsTTN7MKXlMBV4L+W0+lA2NewPLx9yjrv3AZ1AxQjX\nqgA6wrHDryUiImkw5kAxsyLgCeDr7t5F1H01EzgHaAT+5ojU8DCZ2fVmVmdmdc3NzemujojIcWtM\ngWJmuURh8pi7Pwng7k3u3u/uA8A/EY1xADQA01JOrwllDWF/ePmQc8wsBygFWke4VitQFo4dfq0h\n3P0Bd69199rKysqxfFwRETkEY5nlZcDPgLXufm9KeXXKYZ8GVoX9p4CrwsytGUSD76+6eyPQZWYX\nhGteA/wy5ZzEDK7PAr8J4yxLgHlmVh661OYBS8J7S8OxhHMT1xIRkTTIGf0QPgx8CVhpZstD2XeA\nz5vZOYADm4A/AXD31Wa2GFhDNEPsRnfvD+fdADwEFALPhg2iwHrUzDYCbUSzxHD3NjP7HvBaOO67\n7t4W9m8FFpnZ3cAb4RoiIpImFv2xf2Kora31urq6dFdDROSYYmbL3L12tOP0TXkREYmFAkVERGKh\nQBERkVgoUEREJBYKFBERiYUCRUREYqFAERGRWChQREQkFgoUERGJhQJFRERioUAREZFYKFBERCQW\nChQREYmFAkVERGKhQBERkVgoUMZgW2cPO3v70l0NEZGMNpYnNp7w7viPlfx63XZOnjCO06tLUrZi\nppYVEj3RWETkxKZAGYOvfHgG76spY21jF2sbu/jV6m0kHnRZUpDD6dUlzK4q4uQJ45k2YRwnTRjH\ntAmFFBfkprfiIiJHkQJlDD48ayIfnjVx8OedvX2s27ZjMGDWNnbxnysa6dy9d8h55eNyQ7iMG/pa\nPo7qsgJys9XjKCLHDwXKIRifn8N5J5dz3snlQ8o7d+/lvbZdvNe2iy0p28qGTn61aht9Az54bHaW\nUV1aMBgw0yYUMrW8kKll45haXkhVcT45ChwROYYoUGJUWphL6dRSzppaus97/QPOtq4etrTu4r32\nZOi817aLX6/bTkt375Djs7OMySUFTC0vpKYsETbJ1yllhRTkZh+tjyYiMioFylGSnWVRIJQV8kEq\n9nm/Z28/DR27aWjfTUPHburbdw3uv/xOK9u6ekhp4AAwsSh/n8CZUlbIlLICasrGUVKYowkDInLU\nKFAyREFuNqdUFnFKZdF+39/bP8C2zp4hoZN4XdPYxXNrm9jTNzDknPF52Uwtj0ImNWymlEb7k0s1\njiMi8VGgHCNys7OYFgb298fdad25h4b23WztCIETQmdr525WvNdB+66hkwbMYFJxfhQ0pYVUlxYk\nQ6eskOrSQirG55GVpVaOiIxOgXKcMDMmFuUzsSifs6eV7feY3Xv62doZBU609USvnVEr5/m1TfQO\na+XkZWdRXVZAdWkB1SF0qssKmVJawOTSqLVTNi5XXWsiokA5kRTmjdyt5u607dxDY2fPYOg0dvaw\ntbOHxo7dvPpuG9u6eugfNphTkJvFlNKUiQPDJhFMLinQjDWRE4ACRQaZGRVF+VQU5e93phpEs9Va\nunuTYdOxm22dPWztjLrX1jZ20dK9Z8g5iRlrk0OrpjrsV5cWhtcCJmmatMgxT4EiByU7y6gqKaCq\npIBzD3DM8BlriXGdbV09rN3axa/XNtGzd2jXWpZBZXE+k0sLUwIn6l6rLi0YDCRNIhDJXAoUid1o\nM9bcna7dfTR2Ra2cbaFLbVtXD42dPWxs7uZ3G5rZuad/yHlZBlUlBYMz1oZ3sU0pK6QoX/+XFkkX\n/dcnR52ZUToul9JxucyZXHLA43b07I3CJmVMpz68vvFeO8+sbByy+gBEa6tNKSukpjwxTTpspQWD\nLau8HLVyRI4EBYpkrOKCXIoLcpldVbzf9/sHnOYdvYNTpLembA0dPby2qX2f9dUAKsbnURW60KpK\nEt1pobstdLVpYU+Rg6dAkWNWdpYNDvQPX1ctYUfP3sEWTlNXD9s6e9nW1RP2e1jxXgetO/fsc15R\nfs7gOM7kkmgspyala21KWQH5OVr6RiSVAkWOa4lWzqkHaOUA9Pb1s72rd3AMZ1tnythOZw8bmlpo\n2tEz+MiChMri/CFTpCcV5w92q1WVRPtab01OJKMGiplNAx4BqgAHHnD3/2NmE4BfANOBTcCV7t4e\nzrkduBboB2529yWh/DzgIaAQeAa4xd3dzPLD7zgPaAUWuPumcM5C4I5Qnbvd/eFQPgNYBFQAy4Av\nufu+f2qKjCI/J3vEVQggufRNfepKBImlb7Z28fyafb8UCtGCoYlwmVwStXiqBls+URebvhgqxwvz\n4X92DT/ArBqodvfXzayY6B/vK4AvA23u/gMzuw0od/dbzewM4F+B84EpwPPAqe7eb2avAjcDrxAF\nyk/c/VkzuwF4n7v/TzO7Cvi0uy8IoVUH1BKF2TLgPHdvN7PFwJPuvsjM7gdWuPt9I32W2tpar6ur\nO5T7JDK5gnLGAAAJ50lEQVSixMy1ph1Rd1pTV294Dd1rXb00dfawfce+i3zm52RRVVJAZXE+k4rz\nqSzOp7IovBbnM6m4gEklUZmWwZF0MLNl7l472nGjtlDcvRFoDPs7zGwtMBW4HPhoOOxh4AXg1lC+\nyN17gXfNbCNwvpltAkrc/eVQwUeIgunZcM5d4VqPA39v0Z9slwDPuXtbOOc5YL6ZLQIuAr6Q8vvv\nAkYMFJEjJXXm2kjda339AzR399LY2UNT6FLbFsZzWrp72bC9m/9+u3W/kwnysrMGu9dqyhPbuMFx\nncqifM1gk7Q6qDEUM5sOnEvUwqgKYQOwjahLDKKweTnltPpQtjfsDy9PnPMegLv3mVknUVfWYPmw\ncyqADnfv28+1htf5euB6gJNOOmnMn1XkSMjJzgprohWOeFxvXz8t3XvY3tVD846otVPfsZv69qir\n7fm1+z5DB6BsXC6VRfmDLZpEK6eqpGBw1WmtSiBHypgDxcyKgCeAr7t7V2qfbxgHGbnvLE3c/QHg\nAYi6vNJcHZExyc/JHvzS5oEkViSob99NY8dumnf00tzdS/OOXrbv6OX1LR1s39Gzz6oEgw9vG5y1\nFoVNVXFyQkFFUT7Z6l6TgzSmQDGzXKIweczdnwzFTWZW7e6NYZxleyhvAKalnF4TyhrC/vDy1HPq\nzSwHKCUanG8g2a2WOOeF8F6ZmeWEVkrqtUROCKOtSADR2M7OPf2Dz9LZOmxJnAMt+JlYCqeqpGBw\nDGdymFhQVRqFzuSSAkoLNaFAksYyy8uAnwFr3f3elLeeAhYCPwivv0wp/7mZ3Us0KD8beDUMyneZ\n2QVEXWbXAH837FovAZ8FfhNaPUuAvzKzxJcM5gG3h/eWhmMXDfv9IhKYGUX5OcyaVMSsSfsPnr7+\nAbaHVk1TV0+035WcXFDfvovXt7TTtp/v6xTkZg3OYJuaujJBWfLn8VoO54Qxlv+lPwx8CVhpZstD\n2XeIgmSxmV0LbAauBHD31WEG1hqgD7jR3ROLMt1Actrws2GDKLAeDQP4bcBV4VptZvY94LVw3HcT\nA/REEwAWmdndwBvhGiJykHKyswaDYCSp39fZ1jl0Bltjx25eOUBrp7Qwd3D5m+jZOlHgVJdGD3ar\nKs3Xl0SPE6NOGz6eaNqwyJGVaO1sHVwOJ7kO29bOHho7d9Oxa98ZbBOL8gYfEDe4X5z8ubI46mKb\nMD5PXWxpENu0YRGRsUpt7RzoX59de/po7OyhsSN6jk5jRzR1uqW7l5buXjZv2UnLjj3s3tu/z7l5\nOVmDYzmJpXGqwhdGJ4UJBZXFavGkiwJFRI6qcXk5o04mANjZ2zcYMoNdbV3JJXGWv9fBr1b3sGc/\nKxSUj8uNJhSUFFAVJhcMPmOnVCsUHCkKFBHJSOPzcxifn8PJFeMPeIy7075rL42du1MmEwydXLB+\n2w6au3v3GdvJz8mKlsBJeXro4JNFQ6tH06cPjgJFRI5ZZsaE8XlMGJ/HmSMc19c/QEv3Hho7E4+s\nHroI6KvvttHU1bPP83Wysyxq4ZQWDH5RdFJxweAXRhPbxKI8dbOhQBGRE0BOdtbgow4OZGDAad25\nh22Dy+EknyLa1NXD5tZd1G3e//RpiL63MyU85iAxbXpq+bjB/RPhOzsKFBERICvLBlsccyk94HF7\n+wdo7d4TViZILI2TnNm2trGL59fuu/p0Xk4Wk8ICoIkviyb2K8NSOROL8pkwPu+YXZNNgSIichBy\nh7R29h887lFrJ7EqwdawNE5ibGdjczf//XYLXT19+z2/tDCXijB9ujJMnZ6UsiL1pOJoRlv5uLyM\nWoFagSIiEjMzG/xezdnTyg54XM/e6MuiUUtnD607e2nt3kNLd/Ta3N3Lum1dNO/o3W/45IRW1aSS\nAqYmutnCtO3EytRHs6tNgSIikiYFudmcVDGOkyoO/HC3hJ69/UNaOdu7emjaEU2pburqYV3jDn69\ndvs+XW3j87KZWl7IfVefN+pU7cOlQBEROQYU5I7+ZNHUrrbBJ4uGhUDLCnOPeB0VKCIix4mxdrUd\nKcfmVAIREck4ChQREYmFAkVERGKhQBERkVgoUEREJBYKFBERiYUCRUREYqFAERGRWJxQz5Q3s2Zg\n8yGePhFoibE6cVP9Do/qd3hUv8OT6fU72d0rRzvohAqUw2Fmde5+oMdkp53qd3hUv8Oj+h2eTK/f\nWKnLS0REYqFAERGRWChQxu6BdFdgFKrf4VH9Do/qd3gyvX5jojEUERGJhVooIiISCwXKKMxsvpm9\nZWYbzey2dNdnf8xsk5mtNLPlZlaXAfV50My2m9mqlLIJZvacmW0Ir+UZVr+7zKwh3MPlZnZpGus3\nzcyWmtkaM1ttZreE8oy4hyPULyPuoZkVmNmrZrYi1O8vQ3mm3L8D1S8j7t/hUJfXCMwsG1gPfByo\nB14DPu/ua9JasWHMbBNQ6+4ZMY/dzD4CdAOPuPtZoeyHQJu7/yAEc7m735pB9bsL6Hb3/52OOqUy\ns2qg2t1fN7NiYBlwBfBlMuAejlC/K8mAe2jRA9THu3u3meUCvwduAT5DZty/A9VvPhlw/w6HWigj\nOx/Y6O7vuPseYBFweZrrlPHc/UWgbVjx5cDDYf9hon+A0uIA9csY7t7o7q+H/R3AWmAqGXIPR6hf\nRvBId/gxN2xO5ty/A9XvmKdAGdlU4L2Un+vJoP9wUjjwvJktM7Pr012ZA6hy98awvw2oSmdlDuAm\nM3szdImlrUsulZlNB84FXiED7+Gw+kGG3EMzyzaz5cB24Dl3z6j7d4D6QYbcv0OlQDk+/IG7nwP8\nEXBj6NLJWB71s2baX2T3ATOBc4BG4G/SWx0wsyLgCeDr7t6V+l4m3MP91C9j7qG794f/JmqA883s\nrGHvp/X+HaB+GXP/DpUCZWQNwLSUn2tCWUZx94bwuh34d6KuukzTFPreE33w29NcnyHcvSn8Rz4A\n/BNpvoehb/0J4DF3fzIUZ8w93F/9Mu0ehjp1AEuJxicy5v4lpNYvE+/fwVKgjOw1YLaZzTCzPOAq\n4Kk012kIMxsfBkYxs/HAPGDVyGelxVPAwrC/EPhlGuuyj8Q/NMGnSeM9DIO2PwPWuvu9KW9lxD08\nUP0y5R6aWaWZlYX9QqJJNevInPu33/plyv07HJrlNYowde9vgWzgQXf/fpqrNISZzSRqlQDkAD9P\ndx3N7F+BjxKtoNoE3An8B7AYOIloxecr3T0tA+MHqN9HiboaHNgE/ElKf/vRrt8fAL8DVgIDofg7\nROMUab+HI9Tv82TAPTSz9xENumcT/dG82N2/a2YVZMb9O1D9HiUD7t/hUKCIiEgs1OUlIiKxUKCI\niEgsFCgiIhILBYqIiMRCgSIiIrFQoIiISCwUKCIiEgsFioiIxOL/A7SIVtg5SlhJAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f41f927f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864/1864 [==============================] - 0s - loss: 1820347.8517 - val_loss: 3631831.5345\n",
      "Epoch 40/50\n",
      " 500/1864 [=======>......................] - ETA: 0s - loss: 1827820.8750"
     ]
    }
   ],
   "source": [
    "input_file = \"sp500_index_data.csv\"\n",
    "\n",
    "#select which features we are going to examine\n",
    "features = ['Close Price', 'Open Price', 'High Price', 'Low Price',\n",
    "           'Volume', 'MACD', 'CCI', 'ATR', 'BOLL', 'EMA20', 'MA10', 'MTM6', 'MA5',\n",
    "           'MTM12', 'ROC', 'SMI', 'WVAD', 'US Dollar Index', 'Federal Fund Rate']\n",
    "cheat = False\n",
    "cheat_fac = 0.0 \n",
    "normalise = True #normalise data\n",
    "\n",
    "wavelet = False #use wavelet transform\n",
    "auto = False #use autoenconder - if no to both the raw normalised data goes into the lstm or mlp\n",
    "lookback = 4 #if 0 an MLP else the lookback period for an lstm\n",
    "dropout=0.0 #dropout parameter for generalisation tuning\n",
    "\n",
    "#autoencoder settings using relu activation\n",
    "N_EPOCHS=1000\n",
    "neurons = [20,15,15,10] #number of neurons for each layer of the stacked autoencoder\n",
    "BATCH_SIZE = 60\n",
    "LEARNING_RATE = 0.005 \n",
    "\n",
    "#LSTM or MLP Settings\n",
    "BATCH_SIZE_LSTM = 50\n",
    "EPOCHS = 50\n",
    "LSTM_neurons = [50,20]\n",
    "MLP_neurons = [20,10]\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    'lookback':lookback,\n",
    "    'LEARNING_RATE':LEARNING_RATE,\n",
    "    'BATCH_SIZE_LSTM':BATCH_SIZE_LSTM,\n",
    "    'EPOCHS':EPOCHS,\n",
    "    'layer1_2neurons':layer1_2neurons,\n",
    "    'dropout':dropout,\n",
    "    'cheat':cheat,\n",
    "    'cheat_fac':cheat_fac,\n",
    "    'wavelet':wavelet,\n",
    "    'auto':auto,\n",
    "    'N_EPOCHS':N_EPOCHS,\n",
    "    'BATCH_SIZE':BATCH_SIZE,\n",
    "    'LSTM_neurons':LSTM_neurons,\n",
    "    'MLP_neurons':MLP_neurons,\n",
    "    'features':features,\n",
    "    'input_file':input_file,\n",
    "    'normalise':normalise\n",
    "}\n",
    "\n",
    "\n",
    "list_hyper = [0.05, 0.2, 0.005]\n",
    "cross_validate(parameters, list_lr, 'dropout')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
