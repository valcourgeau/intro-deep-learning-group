{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization, ConvLSTM2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wavelet functions\n",
    "# Definining variables for WT\n",
    "# this is the wavelet creation call part...\n",
    "current_level = 2\n",
    "current_mode = 'constant'\n",
    "current_wavelet = pywt.Wavelet('haar')\n",
    "\n",
    "def save_as_csv(data_array, file_name=\"data\"):\n",
    "    output_name = file_name + \".csv\"\n",
    "    np.savetxt(output_name, data_array, delimiter=\",\", fmt='%-7.4f', \n",
    "               header=str(data_array.dtype.names).replace('(',\"\").replace(')',\"\").replace(\"#\", \"\") + \" \",\n",
    "              comments='')\n",
    "    \n",
    "def apply_wt_once(data, wavelet='haar', level=2, mode='constant'):\n",
    "    haar_dwt = pywt.wavedec(data, wavelet=wavelet, level=level, mode=mode)\n",
    "    \n",
    "    # delete the high frequencies from the decomposition\n",
    "    #haar_dwt[1] = np.zeros_like(haar_dwt[1])\n",
    "    haar_dwt[2] = np.zeros_like(haar_dwt[2])\n",
    "  \n",
    "    return pywt.waverec(haar_dwt, mode=mode, wavelet=wavelet)\n",
    "\n",
    "def apply_wt_twice(data, wavelet='haar', level=2, mode='constant'):\n",
    "    wt_results = apply_wt_once(data=data,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "    \n",
    "    return apply_wt_once(data=wt_results,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "\n",
    "def get_accuracy(data1, data2):\n",
    "    return np.std(data1-data2)\n",
    "\n",
    "def apply_wt_twice_on_dataset(dataset, wavelet='haar', level=2, mode='constant'):\n",
    "    #ignored_col_names = ('Ntime', 'time', 'Time') # from the SP500 dataset, might need some tweaking\n",
    "    col_names = dataset.dtype.names\n",
    "    results = np.zeros_like(dataset)\n",
    "\n",
    "    #results[ignored_col_names[0]] = dataset[ignored_col_names[0]]\n",
    "    #if ignored_col_names[1] in dataset.dtype.names:\n",
    "    #    results[ignored_col_names[1]] = dataset[ignored_col_names[1]]\n",
    "    #else:\n",
    "    #    results[ignored_col_names[2]] = dataset[col_names[1]]\n",
    "    \n",
    "    for name in col_names:\n",
    "        #if name not in ignored_col_names:\n",
    "        temp = apply_wt_twice(\n",
    "                            dataset[name],\n",
    "                            wavelet=wavelet,\n",
    "                            level=level,\n",
    "                            mode=mode\n",
    "                             )\n",
    "        results[name] = temp[:len(dataset[name])]\n",
    "    return results\n",
    " \n",
    "def end_to_end_twice_wt_with_csv(input_file_name, output_file_name, wavelet='haar', level=2, mode='constant'):\n",
    "    data_array = np.genfromtxt('data/' + input_file_name, delimiter=',', dtype=float, names=True)\n",
    "    res = apply_wt_twice_on_dataset(data_array,\n",
    "                                wavelet=wavelet,\n",
    "                                level=level,\n",
    "                                mode=mode)\n",
    "    save_as_csv(res, file_name=\"data/data_wt/\" + output_file_name)\n",
    "    res = [list(x) for x in res]\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "#Autoencoder functions\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                       activation, learning_rate = 0.005, l2_reg = 0.0005,\n",
    "                      seed=42):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            activation=activation,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=None, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                #print(\"\\r{}%\".format(100 * iteration // n_batches), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]\n",
    "\n",
    "\n",
    "#normalise dataset pre and post autoencoder input\n",
    "def normalise_dataset(dataset):\n",
    "    n_cols = dataset.shape[1]\n",
    "    norm_dataset = np.zeros_like(dataset)\n",
    "    for i in range(n_cols): \n",
    "        mean = np.sum(dataset[:,i])/len(dataset[:,i])\n",
    "        std_dev = np.dot(dataset[:,i]-mean, dataset[:,i]-mean)/(len(dataset[:,i]) - 1)\n",
    "        std_dev = np.sqrt(std_dev)\n",
    "        norm_dataset[:,i] = (dataset[:,i] - mean) / std_dev\n",
    "\n",
    "    return norm_dataset\n",
    "\n",
    "# conversion code to create lookbacks for lstms\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(trainX, trainY, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(trainX)-look_back-1):\n",
    "        a = trainX[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(trainY[i + look_back-1])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "#Train the stacked autoencoder\n",
    "#then take the latent layer\n",
    "\n",
    "def sae(x_train, N_EPOCHS, BATCH_SIZE,LEARNING_RATE, neurons):\n",
    "    reset_graph()\n",
    "    ACTIVATION = tf.nn.tanh\n",
    "\n",
    "    hidden_output1, W1, b1, W8, b8 = train_autoencoder(x_train, n_neurons=neurons[0], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output2, W2, b2, W7, b7 = train_autoencoder(hidden_output1, n_neurons=neurons[1], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output3, W3, b3, W6, b6 = train_autoencoder(hidden_output2, n_neurons=neurons[2], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    o4, W4, b4, W5, b5 = train_autoencoder(hidden_output3,n_neurons=neurons[3], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "\n",
    "\n",
    "    #Extract the latent outputs from the stacked ae\n",
    "    reset_graph()\n",
    "    activation = ACTIVATION\n",
    "\n",
    "    n_inputs = 1*(x_train.shape[1])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    \n",
    "    hidden1 = activation(tf.matmul(X, W1) + b1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, W2) + b2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, W3) + b3)\n",
    "    hidden4 = activation(tf.matmul(hidden3, W4) + b4)\n",
    "    hidden5 = activation(tf.matmul(hidden4, W5) + b5)\n",
    "    hidden6 = activation(tf.matmul(hidden5, W6) + b6)\n",
    "    hidden7 = activation(tf.matmul(hidden6, W7) + b7)\n",
    "    \n",
    "    \n",
    "    #hidden1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    #hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2)\n",
    "    #hidden3 = tf.nn.relu(tf.matmul(hidden2, W3) + b3)\n",
    "    #hidden4 = tf.nn.relu(tf.matmul(hidden3, W4) + b4)\n",
    "    #hidden5 = tf.nn.relu(tf.matmul(hidden4, W5) + b5)\n",
    "    #hidden6 = tf.nn.relu(tf.matmul(hidden5, W6) + b6)\n",
    "    #hidden7 = tf.nn.relu(tf.matmul(hidden6, W7) + b7)\n",
    "    outputs = tf.matmul(hidden7, W8) + b8\n",
    "\n",
    "    #unnecessary - should be the same as o4 but just to check\n",
    "    with tf.Session() as sess:\n",
    "            latent_val = hidden4.eval(feed_dict={X: x_train})\n",
    "\n",
    "    # Normalise output of SAEs\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "    # Save output in .csv file\n",
    "    latent_out = pd.DataFrame(latent_val) #latent_val\n",
    "    latent_out.to_csv('data/auto_out.csv', index=False)\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    return latent_val  \n",
    "\n",
    "#for the moment just a holder function for running the lstm or mlp\n",
    "#can tweak parameters depending on tests\n",
    "\n",
    "def run_regression(trainX, trainY, testX, testY, look_back, LEARNING_RATE, BATCH_SIZE, EPOCHS, neurons, dropout, decay=0.0):\n",
    "    #reset_graph()\n",
    "    K.clear_session()\n",
    "    # create and fit the LSTM network\n",
    "    \n",
    "    opti_adam = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    \n",
    "    features = trainX.shape[1]\n",
    "    lstm_features = trainX.shape[-1]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #lstm test\n",
    "    if look_back != 0:\n",
    "        for i in enumerate(neurons):\n",
    "            if i[0] != len(neurons)-1:\n",
    "                model.add(LSTM(i[1], input_shape=(look_back, lstm_features),  return_sequences=True, unroll=True))\n",
    "                model.add((Dropout(dropout)))\n",
    "                model.add(BatchNormalization())\n",
    "            else:\n",
    "                model.add(LSTM(i[1]))\n",
    "                model.add((Dropout(dropout)))\n",
    "                model.add(BatchNormalization())\n",
    "    #MLP Test\n",
    "    else:\n",
    "        for i in enumerate(neurons): \n",
    "            model.add(Dense(i[1], input_dim=features, activation= 'relu'))\n",
    "            model.add((Dropout(dropout)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=opti_adam)\n",
    "    history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                        verbose=1, callbacks=[plot_losses, EarlyStopping(monitor='val_loss', patience=5)], \n",
    "                        validation_data=(testX, testY)) #validation_split = 0.1)\n",
    "    \n",
    "\n",
    "    return model, history\n",
    "\n",
    "#this is to create data set inputs to the mlp or lstm\n",
    "def make_reg_data(latent_val, y_train, look_back):\n",
    "\n",
    "    features = latent_val.shape[1]\n",
    "    all_trainX = latent_val \n",
    "\n",
    "    trainX = all_trainX[:round(0.9*len(all_trainX))]\n",
    "    testX = all_trainX[round(0.9*len(all_trainX)):]\n",
    "\n",
    "    all_trainY = np.array(y_train[:-1])\n",
    "    trainY = all_trainY[:round(0.9*len(all_trainY))]\n",
    "    testY = all_trainY[round(0.9*len(all_trainY)):]\n",
    "    \n",
    "    if look_back != 0:\n",
    "        ##### for an mlp dont do the lookback stuff ######\n",
    "        trainX, trainY = create_dataset(trainX, trainY, look_back)\n",
    "        testX, testY = create_dataset(testX, testY, look_back) \n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "#Function to enable live graph of losses as training progresses \n",
    "#note it is called in this cell also and used later\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Live plot for loss during training\n",
    "# Taken from: https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Reset the plot to only have the last 100 epochs\n",
    "        if len(self.x) % 100 == 0:\n",
    "            self.on_train_begin()\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "#examine initial features\n",
    "def make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac):\n",
    "    #including the creation of the wavelet stuff\n",
    "    #returns normalised X, and target y, currently hard coded files\n",
    "    #for targets it assumes a close price for the input file\n",
    "    #also for now hard coded to data/data_wt\n",
    "    \n",
    "    raw_data = pd.read_csv(\"data/\" + input_file)\n",
    "\n",
    "    #set up with complete cheat features to check it can learn - an MLP does what about lSTM\n",
    "    cheat_data = raw_data[features].copy() # raw_data[['Ntime', 'time', 'Close Price','Open Price']].copy()\n",
    "    if cheat:\n",
    "        cheat_data[\"Cheat\"] = raw_data['Close Price'].shift(-1)\n",
    "    cheat_data = cheat_data[:-1] #to keep it clean will take the last off whether cheat or not - because of y's\n",
    "    #print(cheat_data.head())\n",
    "    cheat_data.to_csv('data/cheat.csv', index=False)\n",
    "    \n",
    "    #run Valentin's wavelet stuff\n",
    "    ok = end_to_end_twice_wt_with_csv(\"cheat.csv\", \"cheat500_wt\")\n",
    "\n",
    "    if wavelet:\n",
    "        x_train = np.genfromtxt('data/data_wt/cheat500_wt.csv', delimiter=',', dtype=None, names=True)\n",
    "    else:\n",
    "        x_train = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "    x_train = [list(item) for item in x_train]\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    #set up target and test y's hard coded target price\n",
    "    y_lazy = pd.read_csv(\"data/\"+input_file)['Close Price'] \n",
    "    y_train = y_lazy.shift(-1)\n",
    "    \n",
    "    #because of the shift the last value will be n/a thus...\n",
    "    x_train = x_train[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "\n",
    "    # Normalised dataset\n",
    "    if normalise:\n",
    "        x_train = normalise_dataset(x_train)\n",
    "        \n",
    "    if cheat:\n",
    "        x_train[:,-1] = x_train[:,-1] +np.random.randn(x_train.shape[0])*cheat_fac\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "### Adding goodness of fit measures\n",
    "# MAPE\n",
    "def compute_mape(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    return np.sum(np.abs((y-y_pred) / y)) / len(y)\n",
    "\n",
    "def compute_r(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    return  np.dot(y - mean_pred, y_pred - mean_pred) / np.sqrt(np.sum(np.square(y - mean_pred)) * np.sum(np.square(y_pred - mean_pred)))\n",
    "\n",
    "def compute_theil_u(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    \n",
    "    n = len(y)\n",
    "    temp = np.sqrt(np.sum(np.square(y)) / n)\n",
    "    temp += np.sqrt(np.sum(np.square(y_pred)) / n)\n",
    "    temp = np.sqrt(np.sum(np.square(y - y_pred)) / n) / temp \n",
    "    return temp\n",
    "\n",
    "# Profitability\n",
    "\n",
    "BUY_COST = 0.25/100\n",
    "SELL_COST = 0.45/100\n",
    "\n",
    "def daily_pnl(y_tomorrow, y_today, buy=True, buy_cost=BUY_COST, sell_cost=SELL_COST):\n",
    "    if(buy):\n",
    "        return (y_tomorrow - y_today - (sell_cost*y_tomorrow + buy_cost*y_today)) / y_today\n",
    "    else:\n",
    "        return (y_today - y_tomorrow - (buy_cost*y_today + sell_cost*y_tomorrow)) / y_today\n",
    "    \n",
    "\n",
    "def buy_and_sell(y_truth, y_pred):\n",
    "    assert len(y_pred) == len(y_truth)\n",
    "    n = len(y_pred)\n",
    "    # Vector of 1s for buy, 0s for sell. Approx: no exact predictions...\n",
    "    buy_sell = [1 if y_pred[index+1] > y_truth[index] else -1 if y_pred[index+1] < y_truth[index] else 0 for index in range(n-1)]\n",
    "    ret = [daily_pnl(y_truth[index+1], y_truth[index]) if buy_sell[index] == 1 else\n",
    "           daily_pnl(y_truth[index+1], y_truth[index], buy=False) for index in range(n-1)]\n",
    "    return np.sum(ret)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_set_regressions(params_list, folder_path):\n",
    "    output_list = []   \n",
    "    for param in params_list:\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "        if param['lookback'] != 0:\n",
    "            layers_to_neurons =  param['LSTM_neurons']\n",
    "        else:\n",
    "            layers_to_neurons =  param['MLP_neurons']\n",
    "\n",
    "        #get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "        x_train, y_train = make_raw_data(param['input_file'], param['features'], param['wavelet'], \n",
    "                                         param['normalise'], param['cheat'], param['cheat_fac'])\n",
    "        #then get the output from the autoencoder\n",
    "        if param['auto']:\n",
    "            latent_val = sae(x_train, param['N_EPOCHS'], param['BATCH_SIZE'], \n",
    "                             param['LEARNING_RATE'], param['neurons']) #the output is normalised\n",
    "        else:\n",
    "            latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "            latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "        trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, param['lookback'])   \n",
    "        \n",
    "        \n",
    "        model, history = run_regression(trainX, trainY, testX, testY, \n",
    "                                        param['lookback'], param['LEARNING_RATE'], param['BATCH_SIZE_LSTM'], \n",
    "                                        param['EPOCHS'], layers_to_neurons, param['dropout'])\n",
    "        end_time = start_time - time.time()\n",
    "        predY = model.predict(testX)[:,0]\n",
    "        \n",
    "        performance_metrics = {'MAPE': compute_mape(testY, predY),\n",
    "                              'R': compute_r(testY, predY),\n",
    "                              'theilU': compute_theil_u(testY, predY),\n",
    "                              'buy_and_sell': buy_and_sell(testY, predY),\n",
    "                               'preparation_and_training_time':end_time,\n",
    "                              'testY': np.array(testY),\n",
    "                              'predY': np.array(predY)}\n",
    "        \n",
    "        # Saving everything\n",
    "        model.save(folder_save + param['model_name'] + \".h5\")\n",
    "        np.save(folder_save + param['model_name'] + \".npy\", performance_metrics)\n",
    "        \n",
    "        output_list.append((param, model, history, performance_metrics))\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAD8CAYAAABQFVIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d217d53d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864/1864 [==============================] - 0s - loss: 2235467.6399 - val_loss: 4217031.9852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'BATCH_SIZE': 60,\n",
       "   'BATCH_SIZE_LSTM': 50,\n",
       "   'EPOCHS': 2,\n",
       "   'LEARNING_RATE': 0.005,\n",
       "   'LSTM_neurons': [50, 20],\n",
       "   'MLP_neurons': [20, 10],\n",
       "   'N_EPOCHS': 1000,\n",
       "   'auto': False,\n",
       "   'cheat': False,\n",
       "   'cheat_fac': 0.0,\n",
       "   'dropout': 0.0,\n",
       "   'features': ['Close Price',\n",
       "    'Open Price',\n",
       "    'High Price',\n",
       "    'Low Price',\n",
       "    'Volume',\n",
       "    'MACD',\n",
       "    'CCI',\n",
       "    'ATR',\n",
       "    'BOLL',\n",
       "    'EMA20',\n",
       "    'MA10',\n",
       "    'MTM6',\n",
       "    'MA5',\n",
       "    'MTM12',\n",
       "    'ROC',\n",
       "    'SMI',\n",
       "    'WVAD',\n",
       "    'US Dollar Index',\n",
       "    'Federal Fund Rate'],\n",
       "   'input_file': 'sp500_index_data.csv',\n",
       "   'lookback': 4,\n",
       "   'model_name': 'sp500_wt_nosae_all_features',\n",
       "   'neurons': [20, 15, 15, 10],\n",
       "   'normalise': True,\n",
       "   'wavelet': True},\n",
       "  <keras.models.Sequential at 0x1d22fab0e48>,\n",
       "  <keras.callbacks.History at 0x1d22fc79940>,\n",
       "  {'MAPE': 0.98878947803784956,\n",
       "   'R': 0.042494962813032139,\n",
       "   'buy_and_sell': -146.83874471237823,\n",
       "   'predY': array([ 23.21077347,  23.24357796,  22.97208977,  22.95884514,\n",
       "           22.6099205 ,  22.52478981,  22.99926376,  23.03048897,\n",
       "           22.71011925,  22.61184692,  22.8902626 ,  22.97874641,\n",
       "           23.38278961,  23.4162426 ,  23.53236008,  23.50104332,\n",
       "           23.44564438,  23.46011162,  22.86685181,  22.72524071,\n",
       "           22.18761826,  22.09526825,  21.61898422,  21.43939972,\n",
       "           21.27063942,  21.10668755,  20.96377373,  20.86176109,\n",
       "           20.69848633,  20.62384415,  20.81433868,  20.76767349,\n",
       "           21.09812164,  21.22719193,  21.35295105,  21.3723278 ,\n",
       "           21.86507225,  21.90726089,  21.67385483,  21.61299515,\n",
       "           21.61590576,  21.6668129 ,  21.03456688,  20.90485001,\n",
       "           20.83413315,  20.80659866,  21.16271973,  21.20193672,\n",
       "           21.6474762 ,  21.68908501,  21.95550156,  22.052948  ,\n",
       "           22.08384132,  22.11099815,  22.36819077,  22.49835014,\n",
       "           22.4543457 ,  22.45059204,  22.75850296,  22.78661346,\n",
       "           22.75441551,  22.7885952 ,  22.74885178,  22.82059097,\n",
       "           22.92717552,  22.98250961,  23.09329224,  23.0962677 ,\n",
       "           23.23429108,  23.27151489,  23.28701401,  23.24694252,\n",
       "           23.26552963,  23.34604454,  23.298172  ,  23.39174461,\n",
       "           23.47895813,  23.45175934,  23.46033287,  23.4258213 ,\n",
       "           23.33291817,  23.35661125,  23.26206589,  23.28113556,\n",
       "           23.20755196,  23.24767303,  23.44643021,  23.4383049 ,\n",
       "           23.53371048,  23.50491142,  23.60064888,  23.61694336,\n",
       "           23.58977509,  23.61824417,  23.56018257,  23.61310959,\n",
       "           23.60945511,  23.64083672,  23.28726768,  23.21673203,\n",
       "           23.16588402,  23.2080307 ,  22.9806633 ,  22.95451736,\n",
       "           23.08254623,  23.09350204,  23.35422516,  23.3417778 ,\n",
       "           23.18962479,  23.16022491,  23.16936493,  23.19225883,\n",
       "           22.97360802,  22.94488144,  23.20833206,  23.2556057 ,\n",
       "           23.54215813,  23.48147202,  23.60628319,  23.57903481,\n",
       "           23.5693779 ,  23.59742546,  23.62843323,  23.64634705,\n",
       "           23.72264862,  23.7367115 ,  23.77798843,  23.76926613,\n",
       "           23.48506165,  23.48781586,  23.2097187 ,  23.25848007,\n",
       "           23.1988945 ,  23.18609238,  23.44167137,  23.44052505,\n",
       "           23.57151794,  23.53186417,  22.43646622,  22.16961288,\n",
       "           22.69833755,  22.86582565,  23.54684067,  23.57188416,\n",
       "           23.54203796,  23.47201157,  23.59465408,  23.62752533,\n",
       "           23.88431358,  23.89901352,  23.96501923,  23.99655533,\n",
       "           24.02039337,  24.02216911,  24.05810165,  24.10973358,\n",
       "           24.10429382,  24.13622284,  24.10099983,  24.10129929,\n",
       "           24.01469421,  24.01064301,  24.04784393,  24.01795959,\n",
       "           23.68913651,  23.68598557,  23.98295593,  24.0320015 ,\n",
       "           24.17134857,  24.13108826,  24.05145454,  24.0421257 ,\n",
       "           24.09810066,  24.12602997,  23.98876381,  23.99500084,\n",
       "           24.0580616 ,  24.06982422,  24.0762558 ,  24.06624222,\n",
       "           23.78487396,  23.78798103,  23.81449509,  23.85931396,\n",
       "           23.84345627,  23.84144974,  23.91104507,  23.91758347,\n",
       "           24.0965271 ,  24.06866264,  23.62530518,  23.57003975,\n",
       "           23.35725021,  23.367342  ,  23.4967041 ,  23.5078125 ,\n",
       "           23.56937981,  23.54956055,  23.69695282,  23.69360542,\n",
       "           23.90272522,  23.88346672,  23.75871658], dtype=float32),\n",
       "   'preparation_and_training_time': -7.564012050628662,\n",
       "   'testY': array([ 2063.59,  2047.62,  2052.23,  2012.37,  2021.94,  2043.41,\n",
       "           2073.07,  2041.89,  2005.55,  2021.15,  2038.97,  2064.29,\n",
       "           2060.99,  2056.5 ,  2078.36,  2063.36,  2043.94,  2012.66,\n",
       "           2016.71,  1990.26,  1943.09,  1922.03,  1923.67,  1938.68,\n",
       "           1890.28,  1921.84,  1880.33,  1881.33,  1859.33,  1868.99,\n",
       "           1906.9 ,  1877.08,  1903.63,  1882.95,  1893.36,  1940.24,\n",
       "           1939.38,  1903.03,  1912.53,  1915.45,  1880.05,  1853.44,\n",
       "           1852.21,  1851.86,  1829.08,  1864.78,  1895.58,  1926.82,\n",
       "           1917.83,  1917.78,  1945.5 ,  1921.27,  1929.8 ,  1951.7 ,\n",
       "           1948.05,  1932.23,  1978.35,  1986.45,  1993.4 ,  1999.99,\n",
       "           2001.76,  1979.26,  1989.26,  1989.57,  2022.19,  2019.64,\n",
       "           2015.93,  2027.22,  2040.59,  2049.58,  2051.6 ,  2049.8 ,\n",
       "           2036.71,  2035.94,  2037.05,  2055.01,  2063.95,  2059.74,\n",
       "           2072.78,  2066.13,  2045.17,  2066.66,  2041.91,  2047.6 ,\n",
       "           2041.99,  2061.72,  2082.42,  2082.78,  2080.73,  2094.34,\n",
       "           2100.8 ,  2102.4 ,  2091.48,  2091.58,  2087.79,  2091.7 ,\n",
       "           2095.15,  2075.81,  2065.3 ,  2081.43,  2063.37,  2051.12,\n",
       "           2050.63,  2057.14,  2058.69,  2084.39,  2064.46,  2064.11,\n",
       "           2046.61,  2066.66,  2047.21,  2047.63,  2040.04,  2052.32,\n",
       "           2048.04,  2076.06,  2090.54,  2090.1 ,  2099.06,  2096.96,\n",
       "           2099.33,  2105.26,  2099.13,  2109.41,  2112.13,  2119.12,\n",
       "           2115.48,  2096.07,  2079.06,  2075.32,  2071.5 ,  2077.99,\n",
       "           2071.22,  2083.25,  2088.9 ,  2085.45,  2113.32,  2037.41,\n",
       "           2000.54,  2036.09,  2070.77,  2098.86,  2102.95,  2088.55,\n",
       "           2099.73,  2097.9 ,  2129.9 ,  2137.16,  2152.14,  2152.43,\n",
       "           2163.75,  2161.74,  2166.89,  2163.78,  2173.02,  2165.17,\n",
       "           2175.03,  2168.48,  2169.18,  2166.58,  2170.06,  2173.6 ,\n",
       "           2170.84,  2157.03,  2163.79,  2164.25,  2182.87,  2180.89,\n",
       "           2181.74,  2175.49,  2185.79,  2184.05,  2190.15,  2178.15,\n",
       "           2182.22,  2187.02,  2183.87,  2182.64,  2186.9 ,  2175.44,\n",
       "           2172.47,  2169.04,  2180.38,  2176.12,  2170.95,  2170.86,\n",
       "           2179.98,  2186.48,  2186.16,  2181.3 ,  2127.81,  2159.04,\n",
       "           2127.02,  2125.77,  2147.26,  2139.16,  2139.12,  2139.76,\n",
       "           2163.12,  2177.18,  2164.69,  2146.1 ,  2159.93]),\n",
       "   'theilU': 0.97784198874108885}),\n",
       " ({'BATCH_SIZE': 60,\n",
       "   'BATCH_SIZE_LSTM': 50,\n",
       "   'EPOCHS': 2,\n",
       "   'LEARNING_RATE': 0.005,\n",
       "   'LSTM_neurons': [10, 10],\n",
       "   'MLP_neurons': [20, 10],\n",
       "   'N_EPOCHS': 300,\n",
       "   'auto': True,\n",
       "   'cheat': False,\n",
       "   'cheat_fac': 0.0,\n",
       "   'dropout': 0.0,\n",
       "   'features': ['Close Price',\n",
       "    'Open Price',\n",
       "    'High Price',\n",
       "    'Low Price',\n",
       "    'Volume',\n",
       "    'MACD',\n",
       "    'CCI',\n",
       "    'ATR',\n",
       "    'BOLL',\n",
       "    'EMA20',\n",
       "    'MA10',\n",
       "    'MTM6',\n",
       "    'MA5',\n",
       "    'MTM12',\n",
       "    'ROC',\n",
       "    'SMI',\n",
       "    'WVAD',\n",
       "    'US Dollar Index',\n",
       "    'Federal Fund Rate'],\n",
       "   'input_file': 'sp500_index_data.csv',\n",
       "   'lookback': 4,\n",
       "   'model_name': 'sp500_wt_sae_all_features',\n",
       "   'neurons': [20, 15, 15, 10],\n",
       "   'normalise': True,\n",
       "   'wavelet': True},\n",
       "  <keras.models.Sequential at 0x1d21af8d5f8>,\n",
       "  <keras.callbacks.History at 0x1d232e86c88>,\n",
       "  {'MAPE': 0.99510538903039081,\n",
       "   'R': 0.026275587146091201,\n",
       "   'buy_and_sell': -146.83874471237823,\n",
       "   'predY': array([ 10.65152836,  10.27595043,  10.27027893,  10.41366291,\n",
       "           10.47395706,  10.41367531,  10.52353001,  10.46399689,\n",
       "           10.56143093,  10.69706535,  10.45218945,  10.16486454,\n",
       "            9.87765503,   9.79570198,   9.79656315,   9.734231  ,\n",
       "            9.83950806,  10.05262947,  10.21493053,  10.40245342,\n",
       "           10.47390366,  10.22170353,  10.16586399,   9.84701824,\n",
       "            9.99327278,  10.0582056 ,  10.01200294,   9.77841568,\n",
       "            9.57012844,   9.39113045,   9.16145229,   8.9167757 ,\n",
       "            8.68609428,   8.60617733,   8.73139286,   8.91552544,\n",
       "            8.95385456,   8.85431576,   8.80736256,   8.76547909,\n",
       "            8.78736687,   8.7741003 ,   8.96992207,   9.13078308,\n",
       "            8.92969131,   8.66035557,   8.74689674,   8.80957031,\n",
       "            8.76166248,   8.71010971,   8.87780666,   9.24274445,\n",
       "            9.48668766,   9.79525375,   9.82228756,   9.83244228,\n",
       "            9.97466183,  10.08704662,  10.05899334,   9.95985699,\n",
       "           10.18887138,  10.54456329,  10.49679089,  10.53128719,\n",
       "           10.62736416,  10.70813942,  10.53590488,  10.23175621,\n",
       "           10.45104027,  10.69014263,  10.69290924,  10.60394764,\n",
       "           10.4864254 ,  10.50450993,  10.44591999,  10.44601536,\n",
       "           10.56314373,  10.59551048,  10.69419765,  10.65790081,\n",
       "           10.59761143,  10.60527611,  10.53307438,  10.50451469,\n",
       "           10.4236784 ,  10.32469463,  10.50620461,  10.59349728,\n",
       "           10.63031769,  10.56771374,  10.56214046,  10.61124516,\n",
       "           10.64319038,  10.71069717,  10.58778667,  10.47398853,\n",
       "           10.41790676,  10.41724205,  10.41401291,  10.38626671,\n",
       "           10.2073698 ,   9.86757088,   9.72786617,   9.66205597,\n",
       "            9.65389442,   9.61153603,   9.85353565,  10.07195568,\n",
       "           10.15376663,  10.24244022,  10.1732378 ,  10.09778976,\n",
       "           10.0073452 ,   9.95026493,   9.86430836,   9.69483089,\n",
       "           10.03007889,  10.3586874 ,  10.32640362,  10.20765114,\n",
       "           10.22323227,  10.33134747,  10.37386799,  10.43853855,\n",
       "           10.37595177,  10.23389149,  10.20358849,  10.22239494,\n",
       "           10.24670506,  10.34824467,  10.20845604,   9.97879124,\n",
       "            9.87292004,   9.73845387,   9.89947605,   9.96855164,\n",
       "           10.14394379,  10.27772331,  10.44967937,  10.54594803,\n",
       "           10.17208767,   9.2127533 ,   9.63510704,  10.05830669,\n",
       "           10.15001392,  10.30440712,  10.39920235,  10.66650581,\n",
       "           10.63790226,  10.49071217,  10.42940331,  10.37619686,\n",
       "           10.34864521,  10.45836163,  10.44251919,  10.50626278,\n",
       "           10.5325222 ,  10.52190208,  10.55239391,  10.56311607,\n",
       "           10.6275177 ,  10.63512135,  10.65528584,  10.62032318,\n",
       "           10.39595699,  10.14975166,  10.15509987,  10.12897968,\n",
       "           10.34117126,  10.46355152,  10.45007992,  10.41536713,\n",
       "           10.37227249,  10.36473179,  10.31092548,  10.28579235,\n",
       "           10.28138828,  10.24477005,  10.2016077 ,  10.17386723,\n",
       "           10.06278515,   9.93784904,   9.87315464,   9.84507751,\n",
       "            9.92754745,   9.92745018,   9.90101242,   9.87110996,\n",
       "           10.06460381,  10.17069912,  10.21731949,  10.25338078,\n",
       "           10.33286762,  10.19353485,  10.02023506,   9.77018166,\n",
       "            9.83402729,   9.87604809,   9.88126659,   9.91556263,\n",
       "            9.97306252,  10.05870628,  10.09230423], dtype=float32),\n",
       "   'preparation_and_training_time': -42.871750354766846,\n",
       "   'testY': array([ 2063.59,  2047.62,  2052.23,  2012.37,  2021.94,  2043.41,\n",
       "           2073.07,  2041.89,  2005.55,  2021.15,  2038.97,  2064.29,\n",
       "           2060.99,  2056.5 ,  2078.36,  2063.36,  2043.94,  2012.66,\n",
       "           2016.71,  1990.26,  1943.09,  1922.03,  1923.67,  1938.68,\n",
       "           1890.28,  1921.84,  1880.33,  1881.33,  1859.33,  1868.99,\n",
       "           1906.9 ,  1877.08,  1903.63,  1882.95,  1893.36,  1940.24,\n",
       "           1939.38,  1903.03,  1912.53,  1915.45,  1880.05,  1853.44,\n",
       "           1852.21,  1851.86,  1829.08,  1864.78,  1895.58,  1926.82,\n",
       "           1917.83,  1917.78,  1945.5 ,  1921.27,  1929.8 ,  1951.7 ,\n",
       "           1948.05,  1932.23,  1978.35,  1986.45,  1993.4 ,  1999.99,\n",
       "           2001.76,  1979.26,  1989.26,  1989.57,  2022.19,  2019.64,\n",
       "           2015.93,  2027.22,  2040.59,  2049.58,  2051.6 ,  2049.8 ,\n",
       "           2036.71,  2035.94,  2037.05,  2055.01,  2063.95,  2059.74,\n",
       "           2072.78,  2066.13,  2045.17,  2066.66,  2041.91,  2047.6 ,\n",
       "           2041.99,  2061.72,  2082.42,  2082.78,  2080.73,  2094.34,\n",
       "           2100.8 ,  2102.4 ,  2091.48,  2091.58,  2087.79,  2091.7 ,\n",
       "           2095.15,  2075.81,  2065.3 ,  2081.43,  2063.37,  2051.12,\n",
       "           2050.63,  2057.14,  2058.69,  2084.39,  2064.46,  2064.11,\n",
       "           2046.61,  2066.66,  2047.21,  2047.63,  2040.04,  2052.32,\n",
       "           2048.04,  2076.06,  2090.54,  2090.1 ,  2099.06,  2096.96,\n",
       "           2099.33,  2105.26,  2099.13,  2109.41,  2112.13,  2119.12,\n",
       "           2115.48,  2096.07,  2079.06,  2075.32,  2071.5 ,  2077.99,\n",
       "           2071.22,  2083.25,  2088.9 ,  2085.45,  2113.32,  2037.41,\n",
       "           2000.54,  2036.09,  2070.77,  2098.86,  2102.95,  2088.55,\n",
       "           2099.73,  2097.9 ,  2129.9 ,  2137.16,  2152.14,  2152.43,\n",
       "           2163.75,  2161.74,  2166.89,  2163.78,  2173.02,  2165.17,\n",
       "           2175.03,  2168.48,  2169.18,  2166.58,  2170.06,  2173.6 ,\n",
       "           2170.84,  2157.03,  2163.79,  2164.25,  2182.87,  2180.89,\n",
       "           2181.74,  2175.49,  2185.79,  2184.05,  2190.15,  2178.15,\n",
       "           2182.22,  2187.02,  2183.87,  2182.64,  2186.9 ,  2175.44,\n",
       "           2172.47,  2169.04,  2180.38,  2176.12,  2170.95,  2170.86,\n",
       "           2179.98,  2186.48,  2186.16,  2181.3 ,  2127.81,  2159.04,\n",
       "           2127.02,  2125.77,  2147.26,  2139.16,  2139.12,  2139.76,\n",
       "           2163.12,  2177.18,  2164.69,  2146.1 ,  2159.93]),\n",
       "   'theilU': 0.99026564787230176})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "reg_list = \"reg_list.yml\"\n",
    "folder_save = \"prediction_results/\"\n",
    "\n",
    "with open(\"data/\" + reg_list, 'r') as stream:\n",
    "    try:\n",
    "        regression_dict = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "reg_list = []\n",
    "for reg in regression_dict:\n",
    "    temp_reg = {}\n",
    "    for x in regression_dict[reg]:\n",
    "        temp_reg['model_name'] = reg\n",
    "        for key, value in x.items():\n",
    "            temp_reg[key] = value\n",
    "    reg_list.append(temp_reg)\n",
    "    \n",
    "run_set_regressions(reg_list, folder_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
