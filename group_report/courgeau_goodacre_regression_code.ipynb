{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression approach\n",
    "## Valentin Courgeau 17098662 valentin.courgeau.17@ucl.ac.uk\n",
    "## John Goodacre 13064947 john.goodacre.13@ucl.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization, ConvLSTM2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_level = 2\n",
    "current_mode = 'constant'\n",
    "current_wavelet = pywt.Wavelet('haar')\n",
    "\n",
    "def save_as_csv(data_array, file_name=\"data\"):\n",
    "    output_name = file_name + \".csv\"\n",
    "    np.savetxt(output_name, data_array, delimiter=\",\", fmt='%-7.4f', \n",
    "               header=str(data_array.dtype.names).replace('(',\"\").replace(')',\"\").replace(\"#\", \"\") + \" \",\n",
    "              comments='')\n",
    "    \n",
    "def apply_wt_once(data, wavelet='haar', level=2, mode='constant'):\n",
    "    haar_dwt = pywt.wavedec(data, wavelet=wavelet, level=level, mode=mode)\n",
    "    \n",
    "    # delete the high frequencies from the decomposition\n",
    "    haar_dwt[1] = np.zeros_like(haar_dwt[1])\n",
    "    haar_dwt[2] = np.zeros_like(haar_dwt[2])\n",
    "  \n",
    "    return pywt.waverec(haar_dwt, mode=mode, wavelet=wavelet)\n",
    "\n",
    "def apply_wt_twice(data, wavelet='haar', level=2, mode='constant'):\n",
    "    wt_results = apply_wt_once(data=data,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "    \n",
    "    return apply_wt_once(data=wt_results,\n",
    "                        wavelet=current_wavelet,\n",
    "                        level=current_level,\n",
    "                        mode=current_mode)\n",
    "\n",
    "def get_accuracy(data1, data2):\n",
    "    return np.std(data1-data2)\n",
    "\n",
    "def apply_wt_twice_on_dataset(dataset, wavelet='haar', level=2, mode='constant'):\n",
    "    col_names = dataset.dtype.names\n",
    "    results = np.zeros_like(dataset)\n",
    "\n",
    "    \n",
    "    for name in col_names:\n",
    "        #if name not in ignored_col_names:\n",
    "        temp = apply_wt_twice(\n",
    "                            dataset[name],\n",
    "                            wavelet=wavelet,\n",
    "                            level=level,\n",
    "                            mode=mode\n",
    "                             )\n",
    "        results[name] = temp[:len(dataset[name])]\n",
    "    return results\n",
    " \n",
    "def end_to_end_twice_wt_with_csv(input_file_name, output_file_name, wavelet='db2', level=2, mode='constant'):\n",
    "    data_array = np.genfromtxt('data/' + input_file_name, delimiter=',', dtype=float, names=True)\n",
    "    res = apply_wt_twice_on_dataset(data_array,\n",
    "                                wavelet=wavelet,\n",
    "                                level=level,\n",
    "                                mode=mode)\n",
    "    save_as_csv(res, file_name=\"data/data_wt/\" + output_file_name)\n",
    "    res = [list(x) for x in res]\n",
    "    return np.array(res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#normalise dataset pre and post autoencoder input\n",
    "def normalise_dataset(dataset):\n",
    "    n_cols = dataset.shape[1]\n",
    "    norm_dataset = np.zeros_like(dataset)\n",
    "    for i in range(n_cols): \n",
    "        mean = np.sum(dataset[:,i])/len(dataset[:,i])\n",
    "        std_dev = np.dot(dataset[:,i]-mean, dataset[:,i]-mean)/(len(dataset[:,i]) - 1)\n",
    "        std_dev = np.sqrt(std_dev)\n",
    "        norm_dataset[:,i] = (dataset[:,i] - mean) / std_dev\n",
    "\n",
    "    return norm_dataset\n",
    "\n",
    "# conversion code to create lookbacks for lstms\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(trainX, trainY, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(trainX)-look_back-1):\n",
    "        a = trainX[i:(i+look_back), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(trainY[i + look_back-1])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "#this is to create data set inputs to the mlp or lstm\n",
    "def make_reg_data(latent_val, y_train, look_back):\n",
    "\n",
    "    features = latent_val.shape[1]\n",
    "    all_trainX = latent_val \n",
    "\n",
    "    trainX = all_trainX[:round(0.9*len(all_trainX))]\n",
    "    testX = all_trainX[round(0.9*len(all_trainX)):]\n",
    "\n",
    "    all_trainY = np.array(y_train[:-1])\n",
    "    trainY = all_trainY[:round(0.9*len(all_trainY))]\n",
    "    testY = all_trainY[round(0.9*len(all_trainY)):]\n",
    "    \n",
    "    if look_back != 0:\n",
    "        ##### for an mlp dont do the lookback stuff ######\n",
    "        trainX, trainY = create_dataset(trainX, trainY, look_back)\n",
    "        testX, testY = create_dataset(testX, testY, look_back) \n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "#examine initial features\n",
    "def make_raw_data(input_file, features, wavelet, normalise, cheat, cheat_fac):\n",
    "    #including the creation of the wavelet stuff\n",
    "    #returns normalised X, and target y, currently hard coded files\n",
    "    #for targets it assumes a close price for the input file\n",
    "    #also for now hard coded to data/data_wt\n",
    "    \n",
    "    raw_data = pd.read_csv(\"data/\" + input_file)\n",
    "\n",
    "    #set up with complete cheat features to check it can learn - an MLP does what about lSTM\n",
    "    cheat_data = raw_data[features].copy() # raw_data[['Ntime', 'time', 'Close Price','Open Price']].copy()\n",
    "    if cheat:\n",
    "        cheat_data[\"Cheat\"] = raw_data['Close Price'].shift(-1)\n",
    "    cheat_data = cheat_data[:-1] #to keep it clean will take the last off whether cheat or not - because of y's\n",
    "    cheat_data.to_csv('data/cheat.csv', index=False)\n",
    "    \n",
    "    #run Valentin's wavelet stuff\n",
    "    ok = end_to_end_twice_wt_with_csv(\"cheat.csv\", \"cheat500_wt\")\n",
    "\n",
    "    if wavelet:\n",
    "        x_train = np.genfromtxt('data/data_wt/cheat500_wt.csv', delimiter=',', dtype=None, names=True)\n",
    "    else:\n",
    "        x_train = np.genfromtxt('data/cheat.csv', delimiter=',', dtype=None, names=True)\n",
    "\n",
    "    x_train = [list(item) for item in x_train]\n",
    "    x_train = np.array(x_train)\n",
    "\n",
    "    #set up target and test y's hard coded target price\n",
    "    y_lazy = pd.read_csv(\"data/\"+input_file)['Close Price'] \n",
    "    y_train = y_lazy.shift(-1)\n",
    "    \n",
    "    #because of the shift the last value will be n/a thus...\n",
    "    x_train = x_train[:-1]\n",
    "    y_train = y_train[:-1]\n",
    "\n",
    "    # Normalised dataset\n",
    "    if normalise:\n",
    "        x_train = normalise_dataset(x_train)\n",
    "        \n",
    "    if cheat:\n",
    "        x_train[:,-1] = x_train[:,-1] +np.random.randn(x_train.shape[0])*cheat_fac\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Autoencoder functions\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def train_autoencoder(X_train, n_neurons, n_epochs, batch_size,\n",
    "                       activation, learning_rate = 0.005, l2_reg = 0.0005,\n",
    "                      seed=42):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)\n",
    "\n",
    "        n_inputs = X_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            activation=activation,\n",
    "            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg))\n",
    "\n",
    "        hidden = my_dense_layer(X, n_neurons, name=\"hidden\")\n",
    "        outputs = my_dense_layer(hidden, n_inputs, activation=None, name=\"outputs\")\n",
    "\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))\n",
    "\n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        loss = tf.add_n([reconstruction_loss] + reg_losses)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            n_batches = len(X_train) // batch_size\n",
    "            for iteration in range(n_batches):\n",
    "                sys.stdout.flush()\n",
    "                indices = rnd.permutation(len(X_train))[:batch_size]\n",
    "                X_batch = X_train[indices]\n",
    "                sess.run(training_op, feed_dict={X: X_batch})\n",
    "            loss_train = reconstruction_loss.eval(feed_dict={X: X_batch})\n",
    "            if epoch % 500 == 0:\n",
    "                print(\"\\r{}\".format(epoch), \"Train MSE:\", loss_train)\n",
    "        params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "        hidden_val = hidden.eval(feed_dict={X: X_train})\n",
    "        return hidden_val, params[\"hidden/kernel:0\"], params[\"hidden/bias:0\"], params[\"outputs/kernel:0\"], params[\"outputs/bias:0\"]\n",
    "\n",
    "\n",
    "def sae(x_train, N_EPOCHS, BATCH_SIZE,LEARNING_RATE, neurons):\n",
    "    reset_graph()\n",
    "    ACTIVATION = tf.nn.tanh\n",
    "\n",
    "    hidden_output1, W1, b1, W8, b8 = train_autoencoder(x_train, n_neurons=neurons[0], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output2, W2, b2, W7, b7 = train_autoencoder(hidden_output1, n_neurons=neurons[1], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    hidden_output3, W3, b3, W6, b6 = train_autoencoder(hidden_output2, n_neurons=neurons[2], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "    o4, W4, b4, W5, b5 = train_autoencoder(hidden_output3,n_neurons=neurons[3], \n",
    "                                                       n_epochs=N_EPOCHS, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       learning_rate=LEARNING_RATE,\n",
    "                                                       activation=ACTIVATION)\n",
    "\n",
    "\n",
    "    #Extract the latent outputs from the stacked ae\n",
    "    reset_graph()\n",
    "    activation = ACTIVATION\n",
    "\n",
    "    n_inputs = 1*(x_train.shape[1])\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "    \n",
    "    hidden1 = activation(tf.matmul(X, W1) + b1)\n",
    "    hidden2 = activation(tf.matmul(hidden1, W2) + b2)\n",
    "    hidden3 = activation(tf.matmul(hidden2, W3) + b3)\n",
    "    hidden4 = activation(tf.matmul(hidden3, W4) + b4)\n",
    "    hidden5 = activation(tf.matmul(hidden4, W5) + b5)\n",
    "    hidden6 = activation(tf.matmul(hidden5, W6) + b6)\n",
    "    hidden7 = activation(tf.matmul(hidden6, W7) + b7)\n",
    "    \n",
    "    outputs = tf.matmul(hidden7, W8) + b8\n",
    "\n",
    "    #unnecessary - should be the same as o4 but just to check\n",
    "    with tf.Session() as sess:\n",
    "            latent_val = hidden4.eval(feed_dict={X: x_train})\n",
    "\n",
    "    # Normalise output of SAEs\n",
    "    latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "    # Save output in .csv file\n",
    "    latent_out = pd.DataFrame(latent_val) #latent_val\n",
    "    latent_out.to_csv('data/auto_out.csv', index=False)\n",
    "    \n",
    "    reset_graph()\n",
    "    \n",
    "    return latent_val  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Adding goodness of fit measures\n",
    "# MAPE\n",
    "def compute_mape(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    return np.sum(np.abs((y-y_pred) / y)) / len(y)\n",
    "\n",
    "def compute_r(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    return  np.dot(y - mean_pred, y_pred - mean_pred) / np.sqrt(np.sum(np.square(y - mean_pred)) * np.sum(np.square(y_pred - mean_pred)))\n",
    "\n",
    "def compute_theil_u(y, y_pred):\n",
    "    assert len(y) == len(y_pred)\n",
    "    \n",
    "    n = len(y)\n",
    "    temp = np.sqrt(np.sum(np.square(y)) / n)\n",
    "    temp += np.sqrt(np.sum(np.square(y_pred)) / n)\n",
    "    temp = np.sqrt(np.sum(np.square(y - y_pred)) / n) / temp \n",
    "    return temp\n",
    "\n",
    "# Profitability\n",
    "\n",
    "# Transaction Costs\n",
    "BUY_COST = 0.05/100\n",
    "SELL_COST = 0.05/100\n",
    "\n",
    "def daily_pnl(y_tomorrow, y_today, buy=True, buy_cost=BUY_COST, sell_cost=SELL_COST):\n",
    "    if(buy):\n",
    "        return (y_tomorrow - y_today - (sell_cost*y_tomorrow + buy_cost*y_today)) / y_today\n",
    "    else:\n",
    "        return (y_today - y_tomorrow - (buy_cost*y_today + sell_cost*y_tomorrow)) / y_today\n",
    "    \n",
    "\n",
    "def buy_and_sell(y_truth, y_pred):\n",
    "    assert len(y_pred) == len(y_truth)\n",
    "    n = len(y_pred)\n",
    "    # Vector of 1s for buy, -1s for sell. Approx: no exact predictions...\n",
    "    buy_sell = [1 if y_pred[index+1] > y_truth[index] else -1 if y_pred[index+1] < y_truth[index] else 0 for index in range(n-1)]\n",
    "    ret = [daily_pnl(y_truth[index+1], y_truth[index]) if buy_sell[index] == 1 else\n",
    "           daily_pnl(y_truth[index+1], y_truth[index], buy=False) for index in range(n-1)]\n",
    "    return ret # np.sum(ret)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_regression(trainX, trainY, testX, testY, look_back, LEARNING_RATE, BATCH_SIZE, EPOCHS, neurons, dropout, decay=1e-5):\n",
    "    #reset_graph()\n",
    "    K.clear_session()\n",
    "    # create and fit the LSTM network\n",
    "    \n",
    "    opti_adam = keras.optimizers.Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay)\n",
    "    \n",
    "    features = trainX.shape[1]\n",
    "    lstm_features = trainX.shape[-1]\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #lstm test\n",
    "    if look_back != 0:\n",
    "        for i in enumerate(neurons):\n",
    "            if i[0] != len(neurons)-1:\n",
    "                model.add(LSTM(i[1], input_shape=(look_back, lstm_features),  return_sequences=True, unroll=True))\n",
    "                model.add((Dropout(dropout)))\n",
    "                model.add(BatchNormalization())\n",
    "            else:\n",
    "                model.add(LSTM(i[1]))\n",
    "                model.add((Dropout(dropout)))\n",
    "                model.add(BatchNormalization())\n",
    "    #MLP Test\n",
    "    else:\n",
    "        for i in enumerate(neurons): \n",
    "            model.add(Dense(i[1], input_dim=features, activation= 'relu'))\n",
    "            model.add((Dropout(dropout)))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer=opti_adam)\n",
    "    \n",
    "    history = model.fit(trainX, trainY, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "              verbose=1, callbacks=[plot_losses], validation_split = 0.1)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "#Function to enable live graph of losses as training progresses \n",
    "#note it is called in this cell also and used later\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Live plot for loss during training\n",
    "# Taken from: https://gist.github.com/stared/dfb4dfaf6d9a8501cd1cc8b8cb806d2e\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        # Reset the plot to only have the last 100 epochs\n",
    "        if len(self.x) % 100 == 0:\n",
    "            self.on_train_begin()\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "\n",
    "def run_set_regressions(params_list, folder_path):\n",
    "    output_list = []   \n",
    "    for param in params_list:\n",
    "        # Start timer\n",
    "        start_time = time.time()\n",
    "        if param['lookback'] != 0:\n",
    "            layers_to_neurons =  param['LSTM_neurons']\n",
    "        else:\n",
    "            layers_to_neurons =  param['MLP_neurons']\n",
    "\n",
    "        #get data - potentially wavelet transformed, and potentially with a cheat and cheat_fac\n",
    "        x_train, y_train = make_raw_data(param['input_file'], param['features'], param['wavelet'], \n",
    "                                         param['normalise'], param['cheat'], param['cheat_fac'])\n",
    "        #then get the output from the autoencoder\n",
    "        if param['auto']:\n",
    "            latent_val = sae(x_train, param['N_EPOCHS'], param['BATCH_SIZE'], \n",
    "                             0.005, param['neurons']) #the output is normalised\n",
    "        else:\n",
    "            latent_val = x_train #this could be raw data or wavelet output depending on our choice\n",
    "            latent_val = normalise_dataset(latent_val)\n",
    "\n",
    "        trainX, trainY, testX, testY = make_reg_data(latent_val, y_train, param['lookback'])   \n",
    "        \n",
    "        model, history = run_regression(trainX, trainY, testX, testY, \n",
    "                                        param['lookback'], param['LEARNING_RATE'], param['BATCH_SIZE_LSTM'], \n",
    "                                        param['EPOCHS'], layers_to_neurons, param['dropout'])\n",
    "        end_time = start_time - time.time()\n",
    "        predY = model.predict(testX)[:,0]\n",
    "        \n",
    "        model.summary()\n",
    "        performance_metrics = {'MAPE': compute_mape(testY, predY),\n",
    "                              'R': compute_r(testY, predY),\n",
    "                              'theilU': compute_theil_u(testY, predY),\n",
    "                              'buy_and_sell': buy_and_sell(testY, predY),\n",
    "                               'preparation_and_training_time':end_time,\n",
    "                              'testY': np.array(testY),\n",
    "                              'predY': np.array(predY),\n",
    "                              'val_loss': history.history['val_loss'],\n",
    "                              'loss': history.history['loss'],\n",
    "                              'testX': np.array(testX),\n",
    "                              'trainX': np.array(trainX),\n",
    "                              'trainY': np.array(trainY)}\n",
    "        \n",
    "        # Saving everything\n",
    "        model.save(folder_save + param['model_name'] + \".h5\")\n",
    "        \n",
    "        with open(folder_save + param['model_name'] + \".yml\", 'w') as outfile:\n",
    "            yaml.dump(performance_metrics, outfile, default_flow_style=False)\n",
    "        \n",
    "        output_list.append((param, model, history, performance_metrics))\n",
    "    \n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the fixture file to run regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use YAML fixture file to run examples\n",
    "import yaml\n",
    "reg_list = \"reg_list_test.yml\"\n",
    "folder_save = \"prediction_results/\"\n",
    "\n",
    "with open(\"data/\" + reg_list, 'r') as stream:\n",
    "    try:\n",
    "        regression_dict = yaml.load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "reg_list = []\n",
    "for reg in regression_dict:\n",
    "    temp_reg = {}\n",
    "    for x in regression_dict[reg]:\n",
    "        temp_reg['model_name'] = reg\n",
    "        for key, value in x.items():\n",
    "            temp_reg[key] = value\n",
    "    reg_list.append(temp_reg)\n",
    "    \n",
    "runs = run_set_regressions(reg_list, folder_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
